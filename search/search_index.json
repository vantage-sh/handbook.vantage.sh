{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud Cost Handbook","text":"<p>The Cloud Cost Handbook is a free, open-source, community-supported set of guides meant to help explain the complex pricing of public cloud infrastructure and service providers in easy-to-understand terms. This guide is hosted on GitHub and is open to anyone to contribute their knowledge to the community. Vantage employees will maintain hosting the guide for everyone and ensure that content is relevant and adheres to style guides.</p>"},{"location":"#structure","title":"Structure","text":"<p>This handbook is separated into two different sections as explained below:</p>"},{"location":"#general-concepts","title":"General Concepts","text":"<p>These are general concepts that don't necessarily map directly to a particular service.</p>"},{"location":"#provider-services","title":"Provider Services","text":"<p>Provider services are meant to be the source of truth for explaining not only the pricing mechanics of the service but also to explain potentially nuanced concepts related to costs for that service. The focus of these pages is meant to be on the pricing of these services and not related to the actual management or orchestration of the service itself.</p> <p>A Note About Currency</p> <p>Any listed prices are provided in US Dollars (USD). This is also the default currency AWS uses for billing.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Anyone is welcome to contribute to the Cloud Cost Handbook by issuing a pull request on the GitHub repo. </p> <p> Contribute</p>"},{"location":"#slack-community","title":"Slack Community","text":"<p>Generally interested in cloud concepts and associated costs? Join us in our public Slack community. We have a channel devoted to the handbook named <code>#cloud-costs-handbook</code> where you're welcome to hang out, ask questions, or spark conversation. </p> <p> Slack Community</p>"},{"location":"aws/concepts/autoscaling/","title":"Auto Scaling","text":"<p>The best way to optimize costs in the cloud is to not spend it in the first place. Enter Auto Scaling. Auto Scaling leverages the elasticity of the cloud to dynamically provision and remove capacity based on demand. That means that as demands decrease, Auto Scaling will automatically scale down resources and enable you to save on costs accordingly. </p> <p>Auto Scaling applies to a variety of different services, some of which are described in more detail below. If you're looking for EC2 Auto Scaling concepts, please see the AWS EC2 service page for the Auto Scaling section.</p>"},{"location":"aws/concepts/autoscaling/#application-auto-scaling","title":"Application Auto Scaling","text":"<p>For other resources in AWS, Application Auto Scaling provides the ability to adjust provisioned resources.</p> <p>Application Auto Scaling supports the following services:</p> <ul> <li>AppStream 2.0 fleets</li> <li>Aurora replicas</li> <li>Amazon Comprehend document classification and entity recognizer endpoints</li> <li>DynamoDB tables and global secondary indexes</li> <li>Amazon Elastic Container Service (ECS) services</li> <li>ElastiCache for Redis clusters (replication groups)</li> <li>Amazon EMR clusters</li> <li>Amazon Keyspaces (for Apache Cassandra) tables</li> <li>Lambda function provisioned concurrency</li> <li>Amazon Managed Streaming for Apache Kafka (MSK) broker storage</li> <li>Amazon Neptune clusters</li> <li>SageMaker endpoint variants</li> <li>SageMaker inference components</li> <li>SageMaker Serverless provisioned concurrency</li> <li>Spot Fleet requests</li> <li>Custom resources that are provided by your own applications or services.</li> </ul>"},{"location":"aws/concepts/autoscaling/#auto-scaling-strategies","title":"Auto Scaling Strategies","text":"<p>There are various methods by which Auto Scaling can occur. These are listed below in no particular order:</p> <ul> <li>Target Scaling adds or removes capacity to keep a metric as close to a specific value as possible. For example, a target average CPU utilization of 50% across a set of ECS Tasks. If CPU utilization gets too high, nodes are added. If CPU utilization gets too low, nodes are removed.</li> <li>Step Scaling will adjust capacity up and down by dynamic amounts depending on the magnitude of a metric.</li> <li>Scheduled Scaling will adjust minimum and maximum capacity settings on a schedule.</li> <li>Simple Scaling will add or remove EC2 instances from an Auto Scaling Group when an alarm is in an alert state.</li> <li>Predictive Scaling can leverage historical metrics to preemptively scale EC2 workloads based on daily or weekly trends.</li> <li>Manual Scaling is possible with EC2 instances if teams need to intervene with an Auto Scaling Group. This allows you to manually adjust the Auto Scaling target without any automation. </li> </ul>"},{"location":"aws/concepts/autoscaling/#other-considerations","title":"Other Considerations","text":"<p>Adding capacity is generally an easy process. For compute, it's just a matter of launching new workers from static images or automated standup processes.</p> <p>Reducing capacity can be tricky depending on the application. Web applications generally have their requests clean up to prepare for termination within 30 seconds. Load balancers are often used to drain requests off instances and then terminate the instances \"cleanly\". Queue/batch workers, on the other hand, need to be done with their work, or stash their work somewhere before the node can be terminated. Otherwise, requests and/or data can be lost or incomplete.</p> <p>DynamoDB Provisioned Capacity has restrictions regarding how frequently it can be reduced (four times per day at any time, plus any time when there hasn't been a reduction in the last hour). There are no restrictions regarding increasing capacity. Tables and Secondary Indexes are managed/scaled independently.</p> <p>Scaling cooldown can be the trickiest part of the process. It's generally best to aggressively scale up/out and conservatively scale down/in. A long cooldown process might be necessary when scaling out an application with a long startup process, but it can also block future scale out events, resulting in application instability. Scaling policies should be regularly evaluated and tuned.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/credits/","title":"Credits on AWS","text":"<p>Most public cloud infrastructure and service providers have a concept of credits. Credits are incentives typically given to customers opening up new accounts to attract them to build upon their platform. They allow you to build, learn, and integrate into providers without having to spend money right away. </p> <p>Credit allotments usually are around $5,000 or $10,000 depending on the provider, but can be as high as $100,000.</p>"},{"location":"aws/concepts/credits/#startup-credits-across-clouds","title":"Startup Credits Across Clouds","text":"<p>One strategy that is often used for especially cost-conscious startups utilizing public cloud infrastructure providers, who have the ability to easily move workloads, is to receive credits from multiple providers and run workloads across different providers until credits expire across all of them. So for example, a startup may get $10,000 of AWS credits and $10,000 of GCP credits. A subset of customers will run their application on AWS until their $10,000 is completely utilized then migrate to GCP to use up $10,000 worth of credits there to get $20,000 in total free usage.</p> <p>Typically, this is advised against because the operational overhead of running workloads across multiple clouds typically isn't worth it. The use cases that this tends to work for are very transferable or ephemeral workloads such as training models on GPUs or running containers with no associated state. </p>"},{"location":"aws/concepts/credits/#credit-expiration","title":"Credit Expiration","text":"<p>It's important to note that credits typically have a lifecycle tied to them that causes them to expire. Oftentimes, this catches customers by surprise. Usually, credits are granted on a one-year basis, which means if you have remaining credits that aren't utilized by the expiration term, they're automatically removed from your account. It's important to keep track of your credit expiration dates, so you are not caught off-guard.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/io-operations/","title":"I/O Operations (IOPS) on AWS | Cloud Cost Handbook","text":""},{"location":"aws/concepts/io-operations/#inputoutput-operations","title":"Input/Output Operations","text":"<p>Input/output operations per second (IOPS) are a relatively low-level unit in AWS for measuring disk performance. The maximum size of an IOP is 256 KiB for SSD volumes and 1 MiB for HDD volumes. 1 GiB of storage is worth 3 IOPS, so a 1,000 GiB EBS Volume has 3,000 IOPS available. When using these volume types you are charged for the amount of provisioned IOPS even if you don't fully utilize them.</p> <p>As indicated on the EBS page:</p> <p>Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9% of the time.</p> <p>The performance consistency between a Provisioned IOPS volume and a general purpose (<code>gp2</code>, <code>gp3</code>), throughput optimized (<code>st1</code>), or cold HDD (<code>sc1</code>) is going to be better for both random and sequential disk access. Note that for operations with large and sequential accesses, provisioned IOPS are likely less efficient than a <code>st1</code> volume.</p>"},{"location":"aws/concepts/io-operations/#iops-considerations","title":"IOPS Considerations","text":"<ul> <li>Volume Type: There are multiple volume types with different impacts on IOPS.</li> <li>I/O Demand: Most likely the workload has a bursty demand pattern, where consistently high throughput is not as important as meeting spikes of demand. As the workload deviates from this, provisioned IOPS become more important.</li> <li>Throughput Limits: The instance will have an upper limit of throughput it can support. For example, an i2.xlarge can support up to 62,500 IOPS. If the number of Provisioned IOPS is even higher than this limit, it's a waste, because the instance cannot use them all up.</li> </ul>"},{"location":"aws/concepts/io-operations/#optimal-provisioned-iops","title":"Optimal Provisioned IOPS","text":"<p>The most common cost waste with IOPS is having too many of them. It is commonly believed that the key to RDS is to have some amount of Provisioned IOPS. Luckily, we don't have to guess.</p> <p>AWS suggests inspecting the <code>VolumeQueueLength</code> metric for CloudWatch. This metric is reported as IOPS, which means the formula is simple: if <code>VolumeQueueLength</code> is greater than the number of provisioned IOPS and latency is an issue, then you should consider increasing the number of provisioned IOPS.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/regions/","title":"Regions Pricing","text":"<p>Pricing for public cloud infrastructure providers typically varies by geographic region. Depending on the nature of your applications, you may not have a choice but to be located as close to your users as possible for latency purposes. That being said, it is worth looking at pricing on a per region basis, as there can be significant discounts on a per region basis. </p> <p>The Instances pricing tool has prices for popular AWS services in all regions. To see a list of AWS regions, consult this reference list of AWS regions.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/reserved-instances/","title":"Reserved Instances","text":"<p>Reserved Instances (RIs) are one of the most popular and high-impact cost-reduction methods you can leverage for cutting your bill. Reserved Instances give you the ability to pay upfront for certain AWS services to receive a discount. As a result, if you are able to profile usage across your AWS account and know that you'll hit certain usage levels, Reserved Instances can typically save you money. </p> <p>Reserved Instances are available to a variety of AWS services such as EC2, ElastiCache, and RDS. AWS Billing automatically applies your Reserved Instance discounted rate when attributes of your instance usage match attributes of an active Reserved Instance. For general compute usage (EC2, Fargate, etc.), Savings Plans are always preferred to Reserved Instances, since they give you the same discount but are more flexible across all compute. </p> <p>It's important to note that Reserved Instances aren't actually separate instances. They are merely financial instruments that you buy and are automatically applied to your account. As a result, you can continue to spin up and use On-Demand Instances and purchase Reserved Instances concurrently. As On-Demand Instances match your Reserved Instance attributes, you'll automatically receive discounts. </p>"},{"location":"aws/concepts/reserved-instances/#reserved-instance-term","title":"Reserved Instance Term","text":"<p>AWS gives different discounts depending on the term that you pay upfront for. You can yield greater savings for paying upfront for longer terms, but lose flexibility as a result. We find that smaller customers just getting started in their infrastructure journey tend to prefer 1-Year Reserved Instances, whereas more mature organizations will leverage 3-Year Reserved Instances for the greatest savings as they can more accurately model and predict their usage. </p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/rightsizing/","title":"Rightsizing","text":"<p>Rightsizing is a term used for identifying and augmenting certain resources for greater utilization and potential cost savings. Typically, rightsizing occurs when you're over-provisioned, and can apply to a variety of services, with some examples below:</p> <ul> <li>EC2 Instances: Oftentimes, customers will choose one EC2 Instance that is over-allocated in terms of the amount of vCPU and GBs of RAM it is allocated. As a result, customers may be paying more on a per EC2 Instance basis. Customers who are able to identify opportunities for rightsizing EC2 Instances can typically save significantly, especially if the EC2 Instance type chosen represents a large pool of instances. </li> <li>EBS Volumes: EBS Volumes are frequently a large cost driver for many organizations, and are often heavily under-utilized. EBS charges you for the amount of storage you have allocated, not what you use, so it's important to keep an eye on volume utilization to rightsize and save accordingly. </li> <li>RDS Instances: RDS Instances are similar to EC2 Instances in that they're often overprovisioned, but rarely utilized appropriately. While RDS rightsizing can result in significant cost savings, databases tend to be one of the services that make sense to leave overprovisioned to accommodate growth. Also, downtime for a database during a rightsizing process may ultimately not be worth the cost to your organization. </li> <li>Container Services: ECS, Fargate, and EKS allow you to run services of containers on a pool of underlying EC2 instances either managed by you or managed by AWS if you're using Fargate. Container Services are some of the hardest services to appropriately rightsize but can represent significant saving opportunities, especially for AWS Fargate. </li> </ul> <p>The first step in rightsizing is to have monitoring and observability in place to even know what your utilization is for these various services. Assuming you feel confident in your usage patterns and how they relate to utilization, your organization can begin to make some decisions for potential areas to rightsize. </p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/savings-plans/","title":"Savings Plans","text":"<p>Savings Plans are a flexible pricing model offering discounted prices compared to On-Demand pricing, in exchange for a specific usage commitment. Savings Plans are typically the highest impact, lowest effort way of realizing savings on your AWS account. They are roughly the same concept as Reserved Instances but offer greater flexibility as they (1) can be utilized across multiple compute services (i.e. EC2 and Fargate) and (2) you aren't locked into a specific instance family. Similar to Reserved Instances, there are greater discounts for prepaying for a longer term.</p> <p>After purchasing a Savings Plan, AWS Billing will automatically apply savings as corresponding on-demand resources match the conditions of your Savings Plans. Savings Plans are exclusive to Amazon EC2, AWS Lambda, and AWS Fargate usage. Machine Learning Savings Plans (sometimes called SageMaker Savings Plans) are available for Sagemaker. Typically, customers will use Savings Plans for these services and Reserved Instances for other services that aren't covered such as RDS and ElastiCache.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/concepts/tags/","title":"Tagging Resources","text":"<p>Tags are one of the most powerful (though often overlooked) tools that can assist with your ability to observe and allocate cloud costs related to public cloud infrastructure providers like AWS, Azure, and GCP. While different accounts can be useful for separating resources and costs across different environments (production, staging, QA, test, etc.) or teams/business units, tags are helpful for segmenting costs related to your application. We encourage customers to adopt tagging strategies as early on as possible in their organizations. Similar to an effective unit testing suite, over time tags can give you confidence in understanding where your costs are coming from.</p> <p>Tags in AWS consist of two different parts: a <code>key</code> and a <code>value</code>. As a basic example, you can imagine an example <code>key</code> with the value of \"service\" and a <code>value</code> that could be \"front-end\", \"back-end\", \"search\" or \"cache\". Upon assigning tags to resources, you can get greater visibility into where your costs are coming from. Instead of seeing how your costs are trending in aggregate, you can see how each part of your application is growing, assuming you've leveraged tags correctly. Additionally, tags can be part of your existing workflows and are typically very easy to accommodate in infrastructure-as-code configuration files such as CloudFormation or Terraform.</p> <p>Tags, at their core, are metadata attached to cloud resources. They serve as markers, providing context and categorization. Beyond just identification, tags play a pivotal role in:</p> <p>Cost Allocation: Understand which department, project, or application consumes resources and incurs costs.</p> <p>Cost Optimization: Identify underutilized resources and make informed decisions about scaling or termination.</p> <p>Forecasting: Predict future expenses by analyzing tagged resource consumption.</p> <p>Resource Management: Efficiently manage, search, and filter resources based on specific criteria.</p> <p>Security and Compliance: Ensure resources meet specific security standards or compliance requirements.</p> <p>Operational Clarity: Quickly identify resources during troubleshooting or operational tasks.</p> <p>Alerting: Set event-based notifications for specific resources.</p> <p>Automation: Automate lifecycle management or schedule shutdowns.</p>"},{"location":"aws/concepts/tags/#activating-cost-allocation-tags","title":"Activating Cost Allocation Tags","text":"<p>One of the more generally confusing experiences that customers experience with AWS is that tags are not incorporated into billing reports by default and need to be activated. After you have assigned tags to resources, here are the steps to activate the tags for them to be incorporated into billing data:</p> <p>To activate your tags:</p> <ul> <li>Sign in to the AWS Management Console and open the Billing and Cost Management Console.</li> <li>In the navigation pane, choose <code>Cost Allocation Tags</code>.</li> <li>Select the tags that you want to activate.</li> <li>Choose <code>Activate</code>.</li> </ul> <p>After you create and apply tags to your resources, it can take up to 24 hours for the tags to appear in your reports. Then, after you select your tags for activation, it can take up to 24 hours for the tags to activate.</p>"},{"location":"aws/concepts/tags/#types-of-tags","title":"Types of Tags","text":"<p>Distinguishing between tag types can help in understanding their origin and purpose. AWS-generated tags are system-defined, providing metadata like creation time or AWS service specifics. User-generated tags are defined by users, tailored to organizational needs, and can be as diverse as the business demands.</p> <p>While customization is key, starting with commonly used tags can provide a foundational framework:</p> <ul> <li>Environment: Differentiate between Development, Testing, and Production.</li> <li>Owner: Pinpoint responsibility, aiding in accountability and management.</li> <li>Project: Allocate resources to specific initiatives or campaigns.</li> <li>Cost Center / Business Unit: Facilitate financial reporting and budget allocation.</li> <li>Service: Categorize resources based on the service they support or belong to.</li> <li>Customer: Especially for SaaS providers, understand resource consumption per client.</li> <li>Function: Understand the role or purpose of a resource in the ecosystem.</li> </ul>"},{"location":"aws/concepts/tags/#tagging-strategy","title":"Tagging Strategy","text":"<p>A tagging strategy is not a one-size-fits-all solution. It requires careful consideration of stakeholder needs, cloud complexity, and automation capabilities. We recommend engaging with finance, operations, and development departments to understand their reporting and management needs.</p> <p>A multi-cloud or hybrid cloud environment might require a more nuanced approach for tagging. For example, you may want to align tag values across Datadog and AWS so that you can group costs across providers for a single service together.</p> <p>Utilize tools and scripts to automate tagging for consistency and efficiency. Several types of reports can be built with cloud cost management tools to show you which resources are not tagged so you can make progress.</p> <p>To harness the full potential of tags, maintain consistency with a clear naming convention and stick to it. Tools like AWS Tag Editor or infrastructure-as-code solutions like Terraform, Pulumi, or CloudFormation can help enforce tagging. After the initial setup, make sure to review regularly. As the organization evolves, so will its tagging needs. Periodic reviews ensure relevance. To ensure the stickiness of the strategy, educate and train team members so they understand the importance of tagging and how to do it correctly.</p>"},{"location":"aws/concepts/tags/#implementing-tagging","title":"Implementing Tagging","text":"<p>It\u2019s rare to plan and launch a tagging strategy from scratch. More likely than not, a company already has some infrastructure tagging, and a need to improve this visibility for deeper cost visibility. When implementing a new tagging program that adds or replaces existing tags, we recommend a few collaborative approaches.</p> <p>Firstly, there should be an audit of existing tags. Decide what tags to keep and which to ignore, and measure the accuracy of what tags exist. Then, identify untagged resources. Measure how much of your infrastructure is untagged, and use that to track progress as the program progresses. In the process, you will want to partner with engineering. Clearly communicate new tagging guidelines, and support engineers owning the work to tag infrastructure. Finally, gamify the process. Find ways to recognize or reward teams as tagging work is completed.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/reference/aws-gpu-instances/","title":"AWS GPU Instances | Cloud Cost Handbook","text":"<p>This table is generated by <code>transform_gpus.py</code> in GitHub, with data from the Instances codebase.</p> <p>For more detailed information about matching CUDA compute capability, CUDA gencode, and ML framework version for various NVIDIA architectures, please see this up-to-date resource. The NVIDIA documentation also explains compute capability.</p> GPU Instance Model Compute Capability GPU Count CUDA Cores Memory g2.2xlarge NVIDIA GRID K520 3.0 1 3072 4 g2.8xlarge NVIDIA GRID K520 3.0 4 6144 16 g3s.xlarge NVIDIA Tesla M60 5.2 1 2048 8 g3.4xlarge NVIDIA Tesla M60 5.2 1 2048 8 g3.8xlarge NVIDIA Tesla M60 5.2 2 4096 16 g3.16xlarge NVIDIA Tesla M60 5.2 4 8192 32 g4dn.xlarge NVIDIA T4 Tensor Core 7.5 1 2560 16 g4dn.2xlarge NVIDIA T4 Tensor Core 7.5 1 2560 16 g4dn.4xlarge NVIDIA T4 Tensor Core 7.5 1 2560 16 g4dn.8xlarge NVIDIA T4 Tensor Core 7.5 1 2560 16 g4dn.16xlarge NVIDIA T4 Tensor Core 7.5 1 2560 16 g4dn.12xlarge NVIDIA T4 Tensor Core 7.5 4 10240 64 g4dn.metal NVIDIA T4 Tensor Core 7.5 8 20480 128 p2.xlarge NVIDIA Tesla K80 3.7 1 2496 12 p2.8xlarge NVIDIA Tesla K80 3.7 4 19968 96 p2.16xlarge NVIDIA Tesla K80 3.7 8 39936 192 p3.2xlarge NVIDIA Tesla V100 7.0 1 5120 16 p3.8xlarge NVIDIA Tesla V100 7.0 4 20480 64 p3.16xlarge NVIDIA Tesla V100 7.0 8 40960 128 p3dn.24xlarge NVIDIA Tesla V100 7.0 8 40960 256 g5.xlarge NVIDIA A10G 8.6 1 9616 24 g5.2xlarge NVIDIA A10G 8.6 1 9616 24 g5.4xlarge NVIDIA A10G 8.6 1 9616 24 g5.8xlarge NVIDIA A10G 8.6 1 9616 24 g5.16xlarge NVIDIA A10G 8.6 1 9616 24 g5.12xlarge NVIDIA A10G 8.6 4 38464 96 g5.24xlarge NVIDIA A10G 8.6 4 38464 96 g5.48xlarge NVIDIA A10G 8.6 8 76928 192 p4d.24xlarge NVIDIA A100 8.0 8 55296 320 p4de.24xlarge NVIDIA A100 8.0 8 55296 640 g5g.xlarge NVIDIA T4G Tensor Core 7.5 1 2560 16 g5g.2xlarge NVIDIA T4G Tensor Core 7.5 1 2560 16 g5g.4xlarge NVIDIA T4G Tensor Core 7.5 1 2560 16 g5g.8xlarge NVIDIA T4G Tensor Core 7.5 1 2560 16 g5g.16xlarge NVIDIA T4G Tensor Core 7.5 2 5120 32 g5g.metal NVIDIA T4G Tensor Core 7.5 2 5120 32 g4ad.xlarge AMD Radeon Pro V520 0 1 - 8 g4ad.2xlarge AMD Radeon Pro V520 0 1 - 8 g4ad.4xlarge AMD Radeon Pro V520 0 1 - 8 g4ad.8xlarge AMD Radeon Pro V520 0 2 - 16 g4ad.16xlarge AMD Radeon Pro V520 0 4 - 32 <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/reference/aws-product-codes/","title":"All AWS Product Codes | Cloud Cost Handbook","text":"<p>This table is generated by <code>get_products.py</code> in GitHub. It requires SSM access to run.</p> Product Code 1 AWS Account Management 2 AWS Amplify 3 AWS Amplify Admin 4 AWS App Mesh 5 AWS App Runner 6 AWS AppSync 7 AWS Application Cost Profiler 8 AWS Application Discovery Service 9 AWS Application Migration Service (MGN) 10 AWS Artifact 11 AWS Audit Manager 12 AWS Auto Scaling 13 AWS Backup 14 AWS Backup Gateway 15 AWS Backup Storage 16 AWS Batch 17 AWS Billing Conductor 18 AWS Budgets 19 AWS Certificate Manager 20 AWS Certificate Manager Private Certificate Authority 21 AWS Chatbot 22 AWS Chime Meetings SDK 23 AWS Cloud Map 24 AWS Cloud9 25 AWS CloudFormation 26 AWS CloudHSM 27 AWS CloudHSM 28 AWS CloudShell 29 AWS CloudTrail 30 AWS CodeArtifact 31 AWS CodeBuild 32 AWS CodeCommit 33 AWS CodeDeploy 34 AWS CodePipeline 35 AWS CodeStar 36 AWS CodeStar Notifications 37 AWS Compute Optimizer 38 AWS Config 39 AWS Control Tower 40 AWS Cost Explorer 41 AWS Cost and Usage Report 42 AWS Data Exchange 43 AWS Data Pipeline 44 AWS DataSync 45 AWS Database Migration Service 46 AWS DeepComposer 47 AWS DeepLens 48 AWS DeepRacer 49 AWS Device Farm 50 AWS Direct Connect 51 AWS Directory Service 52 AWS Elastic Beanstalk 53 AWS Elastic Disaster Recovery (DRS) 54 AWS Elemental MediaConnect 55 AWS Elemental MediaConvert 56 AWS Elemental MediaLive 57 AWS Elemental MediaPackage 58 AWS Elemental MediaPackage VOD 59 AWS Elemental MediaStore 60 AWS Elemental MediaStore Data Plane 61 AWS Elemental MediaTailor 62 AWS EventBridge Schemas 63 AWS Fargate 64 AWS Fault Injection Simulator 65 AWS Firewall Manager 66 AWS Global Accelerator 67 AWS Glue 68 AWS Glue DataBrew 69 AWS Ground Station 70 AWS Health APIs And Notifications 71 AWS IAM Access Analyzer 72 AWS IQ 73 AWS Identity and Access Management (IAM) 74 AWS Identity and Access Management Roles Anywhere 75 AWS Import/Export 76 AWS IoT (data plane) 77 AWS IoT 1-Click 78 AWS IoT 1-Click Devices Service 79 AWS IoT Analytics 80 AWS IoT Core 81 AWS IoT Device Advisor 82 AWS IoT Device Defender 83 AWS IoT Device Management 84 AWS IoT Events 85 AWS IoT Events Data 86 AWS IoT Fleet Hub 87 AWS IoT Greengrass 88 AWS IoT Secured Tunneling 89 AWS IoT SiteWise 90 AWS IoT Things Graph 91 AWS IoT TwinMaker 92 AWS Key Management Service 93 AWS Lake Formation 94 AWS Lambda 95 AWS License Manager 96 AWS License Manager User Subscriptions 97 AWS Mainframe Modernization 98 AWS Managed Services 99 AWS Marketplace 100 AWS Marketplace Catalog API 101 AWS Marketplace Commerce Analytics 102 AWS Marketplace Entitlement Service 103 AWS Marketplace Metering Service 104 AWS Migration Hub 105 AWS Migration Hub Refactor Spaces 106 AWS Mobile Service 107 AWS Network Firewall 108 AWS OpsWorks Stacks 109 AWS OpsWorks for Chef Automate 110 AWS OpsWorks for Chef Automate 111 AWS OpsWorks for Puppet Enterprise 112 AWS Organizations 113 AWS Outposts 114 AWS Panorama 115 AWS Personal Health Dashboard 116 AWS Price List Service 117 AWS Private 5G 118 AWS PrivateLink 119 AWS Proton 120 AWS Recycle Bin 121 AWS Resilience Hub 122 AWS Resource Access Manager (RAM) 123 AWS Resource Groups 124 AWS Resource Groups Tagging API 125 AWS RoboMaker 126 AWS S3 Control 127 AWS S3 for Outposts 128 AWS Savings Plans 129 AWS Secrets Manager 130 AWS Security Hub 131 AWS Security Token Service 132 AWS Server Migration Service (SMS) 133 AWS Serverless Application Repository 134 AWS Service Catalog 135 AWS Service Catalog App Registry 136 AWS Shield 137 AWS Signer 138 AWS Single Sign-On 139 AWS Single Sign-On (SSO) OpenID Connect Service 140 AWS Snowball 141 AWS Snowcone 142 AWS Snowmobile 143 AWS Step Functions 144 AWS Storage Gateway 145 AWS Support 146 AWS Support App 147 AWS Systems Manager 148 AWS Systems Manager Incident Manager Contacts 149 AWS Transfer Family 150 AWS Transit Gateway 151 AWS Trusted Advisor 152 AWS VPN 153 AWS WAF 154 AWS WAF Regional 155 AWS Well-Architected Tool 156 AWS X-Ray 157 AWSIdentityStore 158 AWSdlm 159 Alexa for Business 160 Amazon API Gateway 161 Amazon API Gateway V2 162 Amazon AppConfig 163 Amazon AppFlow 164 Amazon AppStream 2.0 165 Amazon Athena 166 Amazon Augmented AI (A2I) 167 Amazon Aurora 168 Amazon Braket 169 Amazon Chime 170 Amazon Chime Messaging 171 Amazon Chime SDK Media Pipelines 172 Amazon Cloud Directory 173 Amazon CloudFront 174 Amazon CloudSearch 175 Amazon CloudWatch 176 Amazon CloudWatch Application Insights 177 Amazon CloudWatch Events 178 Amazon CloudWatch Evidently 179 Amazon CloudWatch Logs 180 Amazon CloudWatch Synthetics 181 Amazon CodeGuru 182 Amazon CodeGuru Reviewer 183 Amazon Cognito 184 Amazon Cognito Identity User Pools 185 Amazon Cognito Sync 186 Amazon Comprehend 187 Amazon Comprehend Medical 188 Amazon Connect 189 Amazon Connect Contact Lens 190 Amazon Connect Customer Profiles 191 Amazon Connect Participant Service 192 Amazon Connect Wisdom Service 193 Amazon Detective 194 Amazon DevOps Guru 195 Amazon DocumentDB (with MongoDB compatibility) 196 Amazon DynamoDB 197 Amazon DynamoDB Accelerator 198 Amazon DynamoDB Streams 199 Amazon EC2 Auto Scaling 200 Amazon EMR Containers 201 Amazon EMR Serverless 202 Amazon ElastiCache 203 Amazon Elastic Block Store (EBS) 204 Amazon Elastic Compute Cloud (EC2) 205 Amazon Elastic Container Registry (ECR) 206 Amazon Elastic Container Registry Public 207 Amazon Elastic Container Service (ECS) 208 Amazon Elastic File System (EFS) 209 Amazon Elastic Inference 210 Amazon Elastic Kubernetes Service (EKS) 211 Amazon Elastic MapReduce (EMR) 212 Amazon Elastic Transcoder 213 Amazon Elasticsearch Service 214 Amazon EventBridge 215 Amazon FSx 216 Amazon FSx for Lustre 217 Amazon FSx for NetApp ONTAP 218 Amazon FSx for OpenZFS 219 Amazon FSx for Windows File Server 220 Amazon FinSpace 221 Amazon FinSpace Beta API 222 Amazon Forecast 223 Amazon Forecast Query 224 Amazon Fraud Detector 225 Amazon GameLift 226 Amazon GameSparks 227 Amazon Glacier 228 Amazon GuardDuty 229 Amazon HealthLake 230 Amazon Honeycode 231 Amazon IVS 232 Amazon IVS Chat 233 Amazon Inspector 234 Amazon Inspector 235 Amazon Kendra 236 Amazon Keyspaces (for Apache Cassandra) 237 Amazon Kinesis Data Analytics 238 Amazon Kinesis Data Firehose 239 Amazon Kinesis Data Streams 240 Amazon Kinesis Video Streams 241 Amazon Lex 242 Amazon Lex Model Building Service 243 Amazon Lex Model Building V2 244 Amazon Lightsail 245 Amazon Location Service 246 Amazon Lookout for Equipment 247 Amazon Lookout for Metrics 248 Amazon Lookout for Vision 249 Amazon Lumberyard 250 Amazon MQ 251 Amazon Machine Learning 252 Amazon Macie 253 Amazon Managed Blockchain 254 Amazon Managed Grafana 255 Amazon Managed Service for Prometheus 256 Amazon Managed Streaming for Apache Kafka 257 Amazon Managed Workflows for Apache Airflow 258 Amazon Mechanical Turk 259 Amazon MemoryDB for Redis 260 Amazon Neptune 261 Amazon Nimble Studio 262 Amazon Personalize 263 Amazon Pinpoint 264 Amazon Pinpoint Email Service 265 Amazon Pinpoint SMS Voice 266 Amazon Pinpoint SMS Voice V2 267 Amazon Pinpoint SMS and Voice Service 268 Amazon Polly 269 Amazon QLDB Session 270 Amazon Quantum Ledger Database (QLDB) 271 Amazon QuickSight 272 Amazon RDS on VMware 273 Amazon Redshift 274 Amazon Rekognition 275 Amazon Relational Database Service (RDS) 276 Amazon Route 53 277 Amazon Route 53 Domains 278 Amazon Route 53 Resolver 279 Amazon SageMaker 280 Amazon SageMaker 281 Amazon SageMaker Feature Store Runtime 282 Amazon Simple Email Service (SES) 283 Amazon Simple Notification Service (SNS) 284 Amazon Simple Queue Service (SQS) 285 Amazon Simple Storage Service (S3) 286 Amazon Simple Workflow Service (SWF) 287 Amazon SimpleDB 288 Amazon Sumerian 289 Amazon Textract 290 Amazon Timestream 291 Amazon Transcribe 292 Amazon Transcribe Medical 293 Amazon Translate 294 Amazon Virtual Private Cloud (VPC) 295 Amazon Voice ID 296 Amazon WorkDocs 297 Amazon WorkMail 298 Amazon WorkMail Message Flow 299 Amazon WorkSpaces 300 Amazon WorkSpaces Application Manager 301 Amazon WorkSpaces Web 302 AmazonApiGatewayManagementApi 303 AmazonAppIntegrationService 304 AmplifyUIBuilder 305 AppConfigData 306 ChimeSDK Identity 307 CloudEndure Disaster Recovery 308 CloudEndure Migration 309 CodeStar Connections 310 EC2 Image Builder 311 Elastic Load Balancing 312 FreeRTOS 313 IoTWirelessConnectivity 314 Managed Streaming for Kafka Connect Engine 315 Migration Hub Strategy Recommendations 316 NetworkManager 317 RDS Data 318 RUMControlPlaneLambda 319 Red Hat OpenShift Service on AWS (ROSA) 320 Redshift Data API Service 321 SSM Incidents 322 Service Quotas 323 SnowDeviceManagement 324 VMware Cloud on AWS <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/reference/aws-regions/","title":"All AWS Regions | Cloud Cost Handbook","text":"<p>This is a list of AWS Regions and their API names from the Instances codebase. Ideally, a <code>boto3</code> script could generate these.</p> Region API Name Region af-south-1 Africa (Cape Town) ap-east-1 Asia Pacific (Hong Kong) ap-south-1 Asia Pacific (Mumbai) ap-northeast-3 Asia Pacific (Osaka) ap-northeast-2 Asia Pacific (Seoul) ap-southeast-1 Asia Pacific (Singapore) ap-southeast-2 Asia Pacific (Sydney) ap-southeast-3 Asia Pacific (Jakarta) ap-northeast-1 Asia Pacific (Tokyo) ca-central-1 Canada (Central) eu-central-1 EU (Frankfurt) eu-west-1 EU (Ireland) eu-west-2 EU (London) eu-west-3 EU (Paris) eu-north-1 EU (Stockholm) eu-south-1 EU (Milan) me-south-1 Middle East (Bahrain) me-central-1 Middle East (UAE) sa-east-1 South America (Sao Paulo) us-east-1 US East (N. Virginia) us-east-2 US East (Ohio) us-west-1 US West (N. California) us-west-2 US West (Oregon) us-gov-west-1 AWS GovCloud (US-West) us-gov-east-1 AWS GovCloud (US-East) <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"aws/services/batch-pricing/","title":"Batch Pricing | Cloud Cost Handbook","text":"<p>AWS Batch Pricing Page</p>"},{"location":"aws/services/batch-pricing/#summary","title":"Summary","text":"<p>AWS Batch combines job scheduling and job execution into one managed service. Example Batch workloads include video rendering, log file ingestion, model training, simulation, and cosmology. Under the hood, Batch provisions EC2 or Fargate instances and executes containerized jobs on them.</p> <p>Users set a range of vCPUs and memory that are needed to execute the job. You can also choose specific instance types, which can be helpful for cost optimizations. For both EC2 and Fargate jobs, it is possible to select on-demand or spot instances. Job execution itself can be managed with scheduling, allocation, and parallelization parameters.</p>"},{"location":"aws/services/batch-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"<p>Batch is free!</p> <p>AWS Batch only consumes the underlying EC2 or Fargate resources, however it does not break these out in the bill. If Batch is pointed at existing EC2 instances, or in other words the Batch Compute Environment is <code>UNMANAGED</code>, the cost of the Batch jobs will be the elapsed time they run on the instance.</p> <p>To view the cost of a job in a <code>MANAGED</code> Batch environment, you must inspect the ECS tasks that are associated with it. You can view the cost of AWS batch jobs through ECS in Cost Reports.</p> <p>Another technique is to add a tag to the compute environment that is created to run the Batch job. This does not allow you to track costs down to the job level.</p>"},{"location":"aws/services/batch-pricing/#where-jobs-should-run","title":"Where Jobs Should Run","text":"<p>One consideration is whether Batch is even the right tool for the job, and if so what compute type is preferred. The table below shows various options for running batch workloads on AWS, and the constraints that come with each.</p> Lambda Fargate (Spot) Fargate EC2 (Spot) EC2 Job Length &lt;15 mins 5 - 10 mins 5 - 10 mins 5 - 45 mins Hours Compute Limits Lambda Runtime Only &lt;4 vCPUs, &lt;30 GiB memory, no GPU &lt;4 vCPUS, &lt;30 GiB mem, no GPU None None Startup Time &lt;1 sec 30 - 90 secs 30 - 90 secs 5 - 15 mins 5 - 15 mins Job is Fault Tolerant No Yes No Yes No"},{"location":"aws/services/batch-pricing/#batch-cost-optimization-tips","title":"Batch Cost Optimization Tips","text":"<p>AWS recommends a few techniques to lower costs for Batch jobs:</p> <ul> <li>The most cost-effective allocation strategy for non-interruptible workloads is <code>BEST_FIT</code>. This strategy is sensitive to capacity constraints however and so an entire workload may have to wait for available machines. To avoid this, <code>BEST_FIT_PROGRESSIVE</code> tries to find the best instances but falls back to less cost-efficient instances that will still complete the job (e.g. have the minimum required number of vCPUs). For Fargate and EC2 Spot workloads, <code>SPOT_CAPACITY_OPTIMIZED</code> uses the same Auto Scaling algorithm as Spot Fleets to get the best price.</li> <li>Use smaller containers and image layers. Loading each container consumes compute time for the job. Furthermore, pulling containers across NAT Gateways will rack up data transfer charges. Prefer PrivateLink for pulling containers.</li> <li>Use multiple availability zones. All things considered, <code>BEST_FIT_PROGRESSIVE</code> will find the cheapest AZ to run the workload in, so do not artificially limit yourself here.</li> </ul> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Sep 1, 2022</p>"},{"location":"aws/services/cloudfront-pricing/","title":"Cloudfront Pricing | Cloud Cost Handbook","text":"<p>Amazon CloudFront Pricing Page</p>"},{"location":"aws/services/cloudfront-pricing/#summary","title":"Summary","text":"<p>Amazon CloudFront is a content delivery network (CDN) service used to distribute and cache traffic from one region to multiple geographic endpoints globally. Every CloudFront distribution includes an origin which is used to pull the original data from. An origin will typically be an S3 bucket or Load Balancer endpoint. The traffic is distributed globally to speed up access to an application that receives visitors from across the globe. CloudFront distributions are billed based on the amount of traffic they request from the origin, distribute to the internet, as well as per request processed. Distribution to the internet is priced differently depending on the region where it is accessed. Regions are grouped into geographic regions. When creating a distribution it is possible to select which regions CloudFront will serve traffic from.</p>"},{"location":"aws/services/cloudfront-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Transfer Out to Internet Distributions are billed per GB of data transferred out of a geographic area to the internet. The prices are tiered and are lower the more traffic is transferred. Regional Data Transfer Out to Origin Distributions are billed per GB of data transfer from the distribution back to the origin. The prices are a flat-rate and dependent on their geographic area. Per Request Distributions are billed per 10,000 requests and are different rates based on whether the request is HTTP or HTTPS. If origin shield is configured there is an additional charge per 10,000 requests and are a standard rate regardless of protocol. Both are priced differently depending on geographic area."},{"location":"aws/services/cloudfront-pricing/#origin-shield","title":"Origin Shield","text":"<p>Origin Shield can be enabled in order to reduce the amount of traffic being served directly from the origin. Origin shields are not available in every region.</p>"},{"location":"aws/services/cloudfront-pricing/#cloudfront-security-savings-bundle","title":"CloudFront Security Savings Bundle","text":"<p>The CloudFront Security Savings Bundle is a simple way to save up to 30% on the CloudFront charges on your AWS bill when you make a 1-year upfront commitment with no service-level configuration changes needed. You're billed in equal installments over the 12 months, starting from the time of purchase. Once you purchase the CloudFront Security Savings Bundle, the savings are automatically applied to your bill. If you're familiar with Savings Plans or Reserved Instances, this is essentially the CloudFront equivalent to those, conceptually speaking. </p> <p>The reason for this being named a bundle is that by making this purchase you also get credits towards the AWS Web Application Firewall (WAF) service. Ten percent of the amount you pay in committed use for a CloudFront Security Savings Bundle will be granted toward AWS WAF. So for example, if you pay $500 for a CloudFront Security Savings Bundle, $50 will also be applied towards AWS WAF. </p>"},{"location":"aws/services/cloudfront-pricing/#custom-pricing","title":"Custom Pricing","text":"<p>For customers who are willing to make certain minimum traffic commits (typically 10TB/month or more) they can contact AWS and negotiate custom discounted rates.</p>"},{"location":"aws/services/cloudfront-pricing/#cloudfront-vs-cloudflare","title":"CloudFront Vs Cloudflare","text":"<p>Cloudflare<sup>1</sup> is an edge network that offers a number of different performance, availability, and security services. One of those services is an edge caching service that offers effectively the same service as Amazon CloudFront. The most important distinction between CloudFront and Cloudflare is not a technical differentiation, but a business model differentiation. CloudFront utilizes a metered pricing model whereby you pay based on the amount of traffic that is served via the CloudFront service.<sup>2</sup> Cloudflare, on the other hand, offers flat-rate pricing for its service without any bandwidth caps.<sup>3</sup> </p> <p>What this means is that as a customer of Cloudflare's Business plan, you can pay $200 per month and deliver unlimited traffic via the Cloudflare CDN. Seems too good to be true? Feel free to browse the official Cloudflare community where this question is asked and answered multiple times.</p>"},{"location":"aws/services/cloudfront-pricing/#considerations","title":"Considerations","text":"<p>Price is not the only consideration that goes into making a decision about whether to utilize CloudFront or a competing CDN service. Performance, availability, user experience, support, and legal compliance are other factors that will factor into the decision to utilize one service over another.</p>"},{"location":"aws/services/cloudfront-pricing/#availability","title":"Availability","text":"<p>In order to offer customers unlimited bandwidth, Cloudflare utilizes service degradation based on their plan levels to prioritize higher-tier customers in the event of service degradation. The two most common service degradations for Cloudflare are either a distributed denial-of-service (DDoS) attack that is overwhelming one or more points-of-presence (PoP) in the network, or a legitimate surge in traffic due to any number of events. </p> <p>When the resources for a PoP are being depleted and service is being degraded, Cloudflare will choose to route traffic for customers out of that location based on the plan level they are subscribed to. Free traffic will be routed away from the PoP first, then Pro, Business, etc. The effect of having traffic routed out of a specific PoP is that users that are closest to the PoP will have some level of service degradation since they will instead have their traffic served from a PoP that is farther away than their most ideal PoP. In locations where the next nearest available PoP is close this degradation will be practically unnoticeable. In locations where the next available PoP is topologically distant, service degradation can potentially be significant.</p>"},{"location":"aws/services/cloudfront-pricing/#technical","title":"Technical","text":"<p>In the scenario that you are utilizing CloudFront and have an Amazon service designated as the origin for the content being served, typically an S3 bucket or maybe EC2 with an attached EBS volume, you should consider that by switching from CloudFront as your CDN to Cloudflare you will incur egress charges for data transfer from AWS to Cloudflare. AWS does not charge customers any egress fees when moving content from an AWS service like S3 or EC2 to CloudFront.<sup>4</sup> The amount of charges will largely be dependent on your particular services cache hit ratio. The higher the cache hit ratio, the less cache misses that will incur AWS egress charges.</p> <p>This practice favors pairing CloudFront with an AWS service as the origin. That being said, most customers with significant  CloudFront traffic will still come out on top by considering a flat-rate priced CDN plan.</p> <p>On top of this, you can also consider moving your content off of an AWS service to a provider in the Bandwidth Alliance. By utilizing the Cloudflare CDN service and a Bandwidth Alliance partner as the content origin, you can take advantage of the flat-rate pricing of the Cloudflare self-serve plans and eliminate all egress costs between Cloudflare and your origin provider of choice. This effectively gives you the same benefit that AWS offers customers, of no egress charges between an AWS service and CloudFront, but with the power of the flat-rate pricing that is available via the Cloudflare self-serve plans. Further details can be found in the S3 service article of the Cloud Cost Handbook.</p>"},{"location":"aws/services/cloudfront-pricing/#sales","title":"Sales","text":"<p>Another side effect of subscribing to a self-serve plan from Cloudflare is that users of these plans are used as part of the sales funnel for the Cloudflare sales team. What this means is that by signing up for Cloudflare you are giving your contact information that can be utilized by the sales team for them to contact you about other Cloudflare services and offerings. </p> <p>The important thing to remember is that as long as you aren't breaking the Cloudflare Terms of Service (ToS) they cannot force you to purchase any additional services.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Aug 8, 2021</p> <ol> <li> <p>This guide is calling special attention to Cloudflare and no other vendors in this space due to the unique offerings that Cloudflare has that no other provider offers. Specifically, they offer self-serve plans with flat-rate pricing and no bandwidth caps. If you are aware of any other services with a similar offering, please submit an issue or pull request and we will update the guide.\u00a0\u21a9</p> </li> <li> <p>Direct link to CloudFront pricing that details metered pricing model: https://aws.amazon.com/cloudfront/pricing/\u00a0\u21a9</p> </li> <li> <p>Direct link to the Cloudflare Terms of Service for the self-serve plans (i.e. the Free, Pro, and Business plans):https://www.cloudflare.com/terms/\u00a0\u21a9</p> </li> <li> <p>\"If you are using an AWS origin, effective December 1, 2014, data transferred from origin to edge locations (Amazon CloudFront \"origin fetches\") will be free of charge.\" https://aws.amazon.com/cloudfront/pricing/\u00a0\u21a9</p> </li> </ol>"},{"location":"aws/services/cloudtrail-pricing/","title":"CloudTrail Pricing | Cloud Cost Handbook","text":"<p>AWS CloudTrail Pricing Page</p>"},{"location":"aws/services/cloudtrail-pricing/#summary","title":"Summary","text":"<p>AWS CloudTrail maintains logs and records of actions and events that occur in your AWS account. It records actions like user and resource activity, API calls, and configuration changes. These logs are useful for troubleshooting operational issues as well as investigating potential security incidents. </p>"},{"location":"aws/services/cloudtrail-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Ingestion and Storage For CloudTrail Lakes, you pay for both ingesting and storing of logs/events from AWS sources and non-AWS sources. Pricing does not differ between source. This pricing includes seven years of storage. Analysis Analysis charges for CloudTrail Lakes are based on the volume of logs you analyze. You are charged per GB of scanned data. CloudTrail Insights analysis charges are based on the Insight type. Events Delivered For CloudTrail trails, pricing is based on the number of data events and management events delivered to S3. Your first management event delivery to S3 is free."},{"location":"aws/services/cloudtrail-pricing/#event-history","title":"Event History","text":"<p>Use the Event history feature directly in the CloudTrail console to view and search historical event and log data. The Event history captures only management events (e.g. if you create or delete S3 buckets). The Event history does not include data events (e.g. if you read or write an S3 object). Event history shows only a 90-day history of the account's activity. You can query across only one Region and a single attribute. Event history has no additional charge. </p>"},{"location":"aws/services/cloudtrail-pricing/#cloudtrail-trails","title":"CloudTrail Trails","text":"<p>Trails collect and store AWS account activity. Trails support the delivery of both management and data events. Unlike the basic Event history, trails contain an event record history that can be greater than 90 days. Additionally, you can specify where to send this activity to in S3 buckets, CloudWatch Logs, or Amazon EventBridge. You have the option to set up Amazon SNS notifications to alert you when CloudTrail adds a new log file to an S3 bucket.  </p>"},{"location":"aws/services/cloudtrail-pricing/#cloudtrail-lake","title":"CloudTrail Lake","text":"<p>A CloudTrail Lake allows you to store and analyze API activity and data logs for up to seven years. You have the ability to view log data from multiple sources and query on numerous records. Compared to the Event history, you can create more customized views and run queries for multiple Regions and attributes. You pay for both ingestion and storage based on \"uncompressed data ingested during the month.\" AWS offers a few suggestions for reducing usage costs, including configuring your options to not ingest future events. </p>"},{"location":"aws/services/cloudtrail-pricing/#cloudtrail-insights","title":"CloudTrail Insights","text":"<p>CloudTrail Insights analyzes management events and reports on unusual or suspicious activity. Events are logged when Insights notices actions that differ from your account's usual event pattern. For example, if Insights starts to see a large increase in deletion API calls, an event is generated. Pricing is based on every 100,000 events analyzed per each Insight type.  </p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Oct 4, 2023</p>"},{"location":"aws/services/cloudwatch-pricing/","title":"CloudWatch Pricing | Cloud Cost Handbook","text":"<p>Amazon CloudWatch Pricing Page</p>"},{"location":"aws/services/cloudwatch-pricing/#summary","title":"Summary","text":"<p>Amazon CloudWatch is a logging, monitoring, and observability service. As with most monitoring and observability tools, the cost of service is based on the amount of data that is collected and stored, as well as a number of other factors. CloudWatch is no different. For most use cases, the largest CloudWatch costs are made up of the number of metrics and logs that a user is ingesting and storing. </p> <p>Oftentimes, CloudWatch is leveraged automatically by other AWS services for metric and log storage. Users are sometimes surprised when they spin up a number of unrelated services which they accounted for during planning, but are then greeted with an accompanying spike in CloudWatch costs that they didn't account for.</p> <p>CloudWatch stores and processes data from an umbrella of different AWS services which means that sometimes it isn't obvious why the overall CloudWatch bill has increased. Diving into subcategory costs can help shed light on which other AWS services are causing CloudWatch costs to increase.</p>"},{"location":"aws/services/cloudwatch-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Custom Metric Storage AWS charges you for the number of custom metrics you store with them per month. CloudWatch's unit pricing is progressive; the first 10,000 metrics tracked is $0.30 per metric per month, the next 240,000 costs $0.10 and so on.<sup>1</sup> This gives users with large numbers of metrics automatic economies of scale as they grow the number of metrics tracked. Note: Pricing is dependant on the region where you store your metrics.<sup>2</sup> CloudWatch API Requests AWS charges you for the following API requests: <code>GetMetricData</code>, <code>GetInsightRuleReport</code>, <code>GetMetricWidgetImage</code>, <code>GetMetricStatistics</code>, <code>ListMetrics</code>, <code>PutMetricData</code>, <code>GetDashboard</code>, <code>ListDashboards</code>, <code>PutDashboard</code> and <code>DeleteDashboards</code>. For most AWS services there is no API charge for sending metrics. A user would normally be charged by the CloudWatch API for ingesting metrics from a non-AWS service or for a third-party infrastructure monitoring/observability tool reading from the API to collect metrics. Pricing is dependant on the region where CloudWatch is deployed.<sup>3</sup> CloudWatch Dashboards AWS charges $3.00 per month per CloudWatch dashboard. CloudWatch Alarms CloudWatch alarms are priced based on the resolution of the alarm (e.g. 60 seconds vs 10 seconds) and if you need to combine multiple alarms together into a more complex alarm like anomaly detection or a composite alarm. Pricing is dependant on the region where CloudWatch is deployed.<sup>3</sup> CloudWatch Logs AWS charges you for two components as it relates to CloudWatch Logs: (1) ingestion and (2) storage. CloudWatch Events AWS charges you for CloudWatch Events which are changes in your AWS environment. For example, you can trigger an event whenever an EC2 instance is created. You are charged a rate per one million events. CloudWatch Contributor Insights Contributor Insights are only available for CloudWatch Logs and DynamoDB. For CloudWatch Logs, Contributor Insights are priced per rule  month, and for every million log events per month that match your rule. For DynamoDB, Contributor Insights are priced per rule per month and for every million DynamoDB Events, which occur when items are read from or written to your DynamoDB table. CloudWatch Canaries Canaries are priced based on the number of runs. Pricing is very specific to region. Be sure to check where you are running your CloudWatch Canaries to be aware of the price for that region."},{"location":"aws/services/cloudwatch-pricing/#cloudwatch-cost-optimizations","title":"CloudWatch Cost Optimizations","text":"<p>By default, CloudWatch Log Groups retain logs indefinitely. However, you can choose a retention period of anywhere from one day to 10 years. After logs expire, they will be deleted which will reduce storage costs. To set a retention period, choose <code>Edit Retention</code> in the CloudWatch console by following these instructions.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Sep 26, 2021</p> <ol> <li> <p>Price is based on US East (Ohio) region as of July 28, 2021. See the footnote below for a comment about pricing per region.\u00a0\u21a9</p> </li> <li> <p>Or at least this is what the CloudWatch pricing page states. If you click through all of the regions the prices are all the same for custom metric storage as of July 28, 2021.\u00a0\u21a9</p> </li> <li> <p>Pricing is fairly uniform across regions. Special regions like Sao Paulo and GovCloud diverge from the standard pricing.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"aws/services/config-pricing/","title":"Config Pricing | Cloud Cost Handbook","text":"<p>Amazon Config Pricing Page</p>"},{"location":"aws/services/config-pricing/#summary","title":"Summary","text":"<p>AWS Config is a service used to record and monitor configuration items in your AWS environment, and then evaluate them using Config rule evaluations. A configuration item is recorded when a monitored resource changes. Resources are AWS entities like EC2 instances, S3 buckets, and IAM roles. You can then apply compliance checks and policies using Config rules.</p> <p>Its most common use case is for compliance monitoring to ensure that your AWS resources adhere to security and operational best practices. However, it is also used for resource administration, managing and troubleshooting configuration changes, and security analysis.</p>"},{"location":"aws/services/config-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Configuration Item Recordings The rate is $0.003 for each configuration item recorded. Config Rule Evaluations See the Config Rule Evaluations section for more information. Conformance Pack Evaluations See the Conformance Pack Evaluations section for more information. S3 Storage Additional S3 costs could occur for snapshots and history files. SNS Charges You will incur additional charges if you opt into SNS notifications. Lambda Charges If you create custom rules that use Lambda functions for evaluation, you will incur Lambda charges based on usage."},{"location":"aws/services/config-pricing/#config-rule-evaluations","title":"Config Rule Evaluations","text":"<p>With Config, you can create Config rules or use the predefined rules (called managed rules) that reflect your desired configurations. These rules are continuously monitored for compliance, with any deviations flagged as noncompliant by Config. You are charged per Config rule evaluation on a tiered model, with evaluations getting less expensive the more there are. After 500,000 evaluations you are no longer charged.</p>"},{"location":"aws/services/config-pricing/#conformance-pack-evaluations","title":"Conformance Pack Evaluations","text":"<p>Conformance pack evaluations occur when a resource is evaluated by a Config rule within a conformance pack. A conformance pack is a \"collection of AWS Config rules and remediation actions.\" Charges are per conformance pack evaluation on a tiered basis, with the cost decreasing as more evaluations occur.</p>"},{"location":"aws/services/config-pricing/#config-cost-optimization-tips","title":"Config Cost Optimization Tips","text":"<p>AWS recommends a few tips for cost optimization:</p> <ul> <li>Only select from the resources needed when configuring Config.</li> <li>Only monitor resources in the regions that are necessary for your specific use case.</li> <li>Set up a lifecycle policy to auto-delete configuration history records after a specified number of days in order to reduce storage costs. </li> <li>Customize conformance packs to ensure there are no duplicate rules.</li> </ul> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Oct 6, 2023</p>"},{"location":"aws/services/detective-pricing/","title":"Detective Pricing | Cloud Cost Handbook","text":"<p>Amazon Detective Pricing Page</p>"},{"location":"aws/services/detective-pricing/#summary","title":"Summary","text":"<p>Amazon Detective is a security service that automatically collects log data from AWS resources and uses machine learning, statistical analysis, and graph theory to build a linked set of data that enables you to investigate potential security issues.</p> <p>Amazon Detective pricing is based on the amount of data ingested for analysis. There is no additional charge for the use of the Amazon Detective service itself, such as investigations, or for storing the findings (Amazon Detective stores the findings for 12 months).</p> <p>It does not currently provide any mechanism to select the type of data to ingest, so all sources will be ingested when enabled. Current sources includes VPC Flow Logs, AWS CloudTrail logs, Amazon Elastic Kubernetes Service (Amazon EKS) audit logs, AWS Security Hub findings, and Amazon GuardDuty findings.</p> <p>CloudWatch Logs are not ingested directly, therefore Amazon Detective does not analyse application level logging and is just for logs generated automatically by AWS resources.</p>"},{"location":"aws/services/detective-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Data Ingestion Amazon Detective pricing is based on the amount of data ingested into the service. Data ingestion is measured in gigabytes (GB) per month and cost per GB varies by region."},{"location":"aws/services/detective-pricing/#organization-accounts","title":"Organization Accounts","text":"<p>When using AWS Organizations there is typically a centralised security or management account which is delegated as the \"Administrator account\" to perform the analysis and view the results of Amazon Detective. The Amazon Detective charges will still be associated with the individual organization account the ingested logs relate to, and not the centralised account which is performing the analysis.</p> <p>Pricing varies by region and is dependent on the origin region of the log data being ingested.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Feb 25, 2025</p>"},{"location":"aws/services/dynamodb-pricing/","title":"DynamoDB Pricing | Cloud Cost Handbook","text":"<p>Amazon DynamoDB Pricing Page</p>"},{"location":"aws/services/dynamodb-pricing/#summary","title":"Summary","text":"<p>DynamoDB is Amazon's primary managed NoSQL database service.</p> <p>It offers single-digit millisecond latency, scales to effectively unlimited requests per second, and has (largely) predictable pricing.</p> <p>DynamoDB, like most NoSQL datastores, differs substantially from relational databases\u2014it can only be queried via primary key attributes on the base table and indexes.</p>"},{"location":"aws/services/dynamodb-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Options Description Billing Mode <code>On Demand, Provisioned Throughput</code> Choose between paying per read/write or per allocated requests per second throughput. Write Type <code>Standard, Transactional</code> Transactional operations allow ACID guarantees at twice the standard cost. Read Type <code>Eventually Consistent, Strongly Consistent, Transactional</code> Dynamo reads are by default Eventually Consistent - when you read from a table, the response might not reflect the results of a recently completed write. Read Operation <code>GetItem, Scan, Query</code> Scans return the entire contents of a table; Queries allow a much faster &amp; cheaper read of a subsection of the table. Indexes <code>None, Local Secondary Index, Global Secondary Index</code> Indexes allow an alternate set of <code>partition_key</code> + <code>sort_key</code> to be used for queries."},{"location":"aws/services/dynamodb-pricing/#billing-mode","title":"Billing Mode","text":"<p>\"Use on-demand until it hurts\" - Alex DeBrie, quoting Jared Short</p> <p>Provisioned Throughput is cheaper if you have a meaningful number of reads/writes distributed evenly across time. Any reads/writes above the provisioned threshold will fail, so it is not well suited to bursty or unpredictable workloads.</p> <p>Provisioned Throughput includes optional Auto Scaling if throughput thresholds are being exceeded (docs).</p> <p>AWS Free Tier includes 25 reads/writes per second of Provisioned Throughput (across any number of tables) but does not include any on-demand mode usage.</p> Billing Mode Unit Unit Definition On-Demand Read Request Unit (RRU) Read two &lt;4KB items, eventually consistent On-Demand Write Request Unit (WRU) Write one &lt;1KB item Provisioned Throughput Read Capacity Unit (RCU) Two reads per second (&lt;4KB items), eventually consistent Provisioned Throughput Write Capacity Unit (WCU) One write per second (&lt;1KB item)"},{"location":"aws/services/dynamodb-pricing/#write-type","title":"Write Type","text":"<p>Standard writes are relatively straightforward and include single-item writes (<code>table.put_item</code>) and batch writes (<code>batch.put_item</code>).</p> <p>Transactions (<code>client.execute_transaction</code>) group up to 25 writes (or reads, updates, or deletes) together and guarantee that they succeed or fail together.</p> <p>For a given write of an item up to 1KB in size:</p> Type Cost Standard Single Item 1 WRU Standard Batch 1 WRU (per item) Transactional 2 WRU (2x) Oversize 4KB Item 4 WRU (4x, size dependent)"},{"location":"aws/services/dynamodb-pricing/#read-type","title":"Read Type","text":"<p>Part of what makes DynamoDB a compelling offering is its hybrid approach to the CAP theorem<sup>1</sup>\u2014it can adjust between eventually and strongly consistent as needed.</p> <p>Wherever acceptable to the business needs and current data modeling, it is faster and cheaper to use eventually consistent reads.</p> <p>That said, some business logic unequivocally dictates strongly consistent reads (e.g. an ATM reading a customer's balance).</p> <p>For a given read of an item up to 4KB in size:</p> Type Cost Eventually Consistent 0.5 RRU Strongly Consistent 1 RRU (2x) Transactional 2 RRU (4x)"},{"location":"aws/services/dynamodb-pricing/#read-operation","title":"Read Operation","text":"<p>Getting a single item is as simple as providing its <code>partition_key</code> (and <code>sort_key</code> if the table has one).</p> <p>Queries, however, are much more involved. NoSQL databases like DynamoDB can require significant upfront data modeling work to enable the query flexibility that SQL-based databases have by default.</p> <p>Scans require reading the entire table and are correspondingly slow and expensive. Wherever possible, avoid scanning Dynamo tables.</p> Type Cost <code>GetItem</code> 1 RRU <code>Query</code> {# of items meeting query logic} RRU <code>Scan</code> {# of items in table} RRU"},{"location":"aws/services/dynamodb-pricing/#indexes","title":"Indexes","text":"<p>Every DynamoDB table has a <code>partition_key</code> (<code>pk</code>) and optional <code>sort_key</code> (<code>sk</code>) specified at the time of creation.</p> <p>Indexes allow alternate partition and sort keys to be used to query items. They may be created at any time and are automatically maintained as new items are written.</p> <p>Indexes can help control costs in two primary ways:</p> <ol> <li> <p>Queries on a new index return fewer unnecessary items (than the alternative/existing query) and thus cost less RRUs.</p> </li> <li> <p>Each index optionally allows a subset of item attributes to be projected to that index. Projecting a subset can save on read costs if items are regularly &gt;4KB, but the projected attribute names+values sum to &lt;4KB.<sup>2</sup></p> </li> </ol> Type Primary Key Attributes Base table Initial <code>pk</code> + optional <code>sk</code> Local Secondary Index (LSI) Initial <code>pk</code> + different <code>sk</code> Global Secondary Index (GSI) Different <code>pk</code> + optional different <code>sk</code> <p>Info</p> <p>The provisioned throughput settings of a global secondary index is separate from those of its base table.</p>"},{"location":"aws/services/dynamodb-pricing/#other","title":"Other","text":"<p>DynamoDB tables can optionally enforce a Time To Live (TTL) on items in the table, such that they expire after that amount of time (guaranteed within +48 hours).</p> <p>Dynamo exposes the time-ordered sequence of item-level changes on a given table via DynamoDB Streams. Reading change data from Streams is slightly cheaper per request than reading the table itself (on pay-per-use BillingMode). The first 2.5M reads per month are free.</p>"},{"location":"aws/services/dynamodb-pricing/#further-reading","title":"Further Reading","text":"<ul> <li>An overview of the architecture of DynamoDB can be found in the DynamoDB Paper</li> <li>You can, if you so choose, use a SQL-like syntax to interface with DynamoDB via PartiSQL support</li> </ul> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 31, 2021</p> <ol> <li> <p>The CAP Theorem is a computer science theorem that observes that a distributed datastore cannot guarantee all three of Consistency, Availability, and Partition Tolerance.\u00a0\u21a9</p> </li> <li> <p>If a query to an index with projection requests attribute values not in the projected values, it will incur twice the normal read cost, as the remaining attribute values must be fetched from the base table.\u00a0\u21a9</p> </li> </ol>"},{"location":"aws/services/ebs-pricing/","title":"EBS Pricing | Cloud Cost Handbook","text":"<p>Amazon EBS Pricing Page</p>"},{"location":"aws/services/ebs-pricing/#summary","title":"Summary","text":"<p>Amazon Elastic Block Storage (EBS) is Amazon's block storage offering that allows you to create volumes, which is the base primitive of everything related to EBS. There are several EBS volume types, each with different capabilities and its own set of pricing. EBS costs are factored into the cost category of EC2-Other on your AWS bill, which can oftentimes complicate understanding where these costs are coming from. </p> <p>It is important to note that you are charged for the amount of provisioned storage not utilized storage. So, for example, if you create a 20GB EBS Volume and only utilize 1GB of it, you are still charged for all 20GB. </p>"},{"location":"aws/services/ebs-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Volume Storage Hours When you create an EBS volume you allocate a certain amount of storage to it. Ultimately, the main cost of an EBS volume is the result of the amount of hours you're using an EBS Volume and the size you allocate. Volume Type EBS has different types of volume types which are documented below. Each volume type has different rates. Provisioned IOPS Certain EBS volume types (io1, io2) allow you to specify an amount of provisioned input/output operations per second (IOPS). When using these volume types you are charged for the amount of provisioned IOPS even if you don't fully utilize them. Amazon EBS Snapshots Amazon EBS Snapshots are a point-in-time copy of your block volume data. EBS Snapshots are stored incrementally, which means you are billed only for the changed blocks stored. EBS Snapshot API Requests EBS charges you for the amount of API calls you make for snapshots. These are charged in increments of thousands of API requests."},{"location":"aws/services/ebs-pricing/#volume-types","title":"Volume Types","text":"<p>Amazon EBS offers different volume types that have different pricing rates and functionality. Each EBS volume type is described below:</p> Volume Type Description General Purpose SSD (gp2, gp3) General Purpose SSD (gp3) volumes offer cost-effective storage that is ideal for a broad range of workloads. Provisioned IOPS (io1, io2) Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9% of the time. Throughput Optimized HDD (st1) Throughput Optimized HDD (st1) volumes offer magnetic storage for frequently accessed data. It is a good fit for large, sequential workloads. Cold HDD (sc1) Cold HDD (sc1) volumes are cheapest in comparison to other volume types. It is intended for infrequently accessed, large, sequential workloads."},{"location":"aws/services/ebs-pricing/#stranded-volumes","title":"Stranded Volumes","text":"<p>Oftentimes, EBS volumes are created in conjunction with other AWS resources such as EC2 instances but are de-coupled from the lifecycle of those other resources. One common pattern we see is that developers will create EC2 instances with EBS volume attached but when they delete the EC2 instance, they assume that the EBS volume is destroyed accordingly. In larger-scale environments with Auto Scaling, this problem can grow significantly as a part of an AWS bill.</p> <p>We recommend that you periodically profile for unattached or stranded EBS volume.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Sep 26, 2023</p>"},{"location":"aws/services/ec2-other-pricing/","title":"EC2-Other Pricing | Cloud Cost Handbook","text":""},{"location":"aws/services/ec2-other-pricing/#summary","title":"Summary","text":"<p>EC2-Other is a category of AWS costs that typically causes the greatest amount of confusion for customers as it doesn't necessarily map to a single AWS service. EC2-Other encompasses the following costs:</p> Usage Type Description EBS Volume Usage Usage for EBS Volumes. EBS Snapshot Usage Usage for EBS Snapshots. CPU Credits from t2/t3/t4g EC2 Instances T-family EC2 Instances can carry potential CPU credit charges as described more below. NAT Gateway Usage Hourly usage for NAT Gateways. Data Transfer Associated with transferring data in and out of EC2 instances. Idle Elastic IP Address Usage AWS charges you for unattached IP addresses. It's typically good hygiene to occasionally monitor for stranded resources and clean them up."},{"location":"aws/services/ec2-other-pricing/#stranded-resources","title":"Stranded Resources","text":"<p>Unused or stranded EBS Volumes and IP Addresses can add up over time, especially if these resources are created automatically as part of an Auto Scaling service where they're spun up but not down. You should consider occasionally auditing your unattached EBS Volumes and IP addresses to see if you can clean them up to save costs.</p>"},{"location":"aws/services/ec2-other-pricing/#what-are-t2t3t4g-cpu-credit-charges","title":"What are t2/t3/T4g CPU Credit Charges?","text":"<p>T2, T3, and T4g instances have a concept of Unlimited mode, whereby you are charged a per vCPU hour for bursting into this CPU usage. If you are leveraging these EC2 Instance Types with <code>unlimited</code> mode enabled, you should consider keeping an eye on these costs. Depending on how much your costs trend here, you may want to consider rightsizing to a different instance type that is allocated additional CPU.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 11, 2021</p>"},{"location":"aws/services/ec2-pricing/","title":"EC2 Pricing | Cloud Cost Handbook","text":"<p>Amazon EC2 Pricing Page</p>"},{"location":"aws/services/ec2-pricing/#summary","title":"Summary","text":"<p>Amazon EC2 (Elastic Cloud Compute) is Amazon\u2019s most popular service and usually one of the top cost centers for most companies. Amazon EC2 allows customers to create virtual private servers and has different pricing depending on the instance type you use. Instance types are grouped into families with varying generations. Each instance type has a different mix of underlying hardware, allocated resources, and as a result: pricing. Additionally, depending on the underlying software running on the EC2 instance you may be charged different rates.</p>"},{"location":"aws/services/ec2-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Instance Type Usage EC2 instance types are billed on one second increments, with a minimum of 60 seconds. For certain instance types with pre-installed software, you are billed in increments of hours. Instance Type Lifecycle EC2 has different lifecycle types - the two most often used are on-demand and spot. These concepts are discussed more in-depth below. Amazon Machine Images (AMI) Depending on the AMI you use (i.e. Linux vs Windows) you potentially pay an additional amount of money on top of the instance type base usage."},{"location":"aws/services/ec2-pricing/#on-demand-vs-spot","title":"On-Demand vs Spot","text":"<p>By default, EC2 instances are launched in On-Demand mode and charged accompanying on-demand rates (which are the most expensive). AWS also offers Spot Instances, which can offer significant cost savings by using unused additional compute capacity. However, Spot tends to only work for fault-tolerant workloads as AWS can pre-empt and terminate these instances within two minutes if need be. </p> <p>Depending on your application's needs, you can consider using Spot Instances for significant cost savings in the event you are comfortable with these instances being terminated. In general, your application's architecture should be comfortable with either (1) there being no Spot Instances available or (2) these instances being terminated. </p>"},{"location":"aws/services/ec2-pricing/#auto-scaling","title":"Auto Scaling","text":"<p>EC2 Auto Scaling is provided by a primitive named Auto Scaling groups. Auto Scaling groups have lifecycle hooks to accommodate complex workflows regarding instance creation or termination and can support multiple instance types or Spot Instances using a Mixed Instance Policy. New instances are added based on a launch template (or launch config). This can be a challenge for organizations without good practices around creating machine images or automation for standing up applications.</p>"},{"location":"aws/services/ec2-pricing/#rightsizing","title":"Rightsizing","text":"<p>Rightsizing refers to the process of ensuring that you're using the proper instance type suited for your application or workload. For example, if you're using the largest instance type in a particular family but not using the CPU, storage, and memory allocated to it fully, you may be overpaying for what you need. Rightsizing is usually a manual process that involves engineering time for looking at a combination of application-level performance metrics like application CPU and memory consumption and infrastructure-related attributes like what kind of underlying CPU powers an instance type. </p>"},{"location":"aws/services/ec2-pricing/#savings-plans","title":"Savings Plans","text":"<p>EC2 instances are covered by AWS Savings Plans. Savings Plans are covered more in-depth as a general concept here. As it relates to EC2, Savings Plans are preferable as they present the same savings as Reserved Instances but aren't constrained to a single instance type. </p>"},{"location":"aws/services/ec2-pricing/#reserved-instances","title":"Reserved Instances","text":"<p>EC2 instances are covered by AWS Reserved Instances. Reserved Instances are covered more in-depth as a general concept here. As it relates to EC2, Reserved Instances aren't preferred as they present the same savings as Savings Plans but are constrained to a single instance type whereas Savings Plans give greater flexibility. </p>"},{"location":"aws/services/ec2-pricing/#instance-type-families","title":"Instance Type Families","text":"<p>EC2 Instance Types are organized into families and each family can have multiple generations. By looking at each instance type you can infer its family and generation from the instance type name. For example, a <code>c5.4xlarge</code> is the <code>c</code> family and <code>5th</code> generation. Below is a table of EC2 Instance Families and simple descriptions:</p> Family Description a AMD(x86) Processor c Compute-optimized d Locally attached spinning HDD f Customizable hardware acceleration with FPGAs g Arm-based AWS Graviton2 processors h Large spinning HDD i Intel(x86) processor with NVMe SSD-backed storage optimized inf Machine-learning inference mac Apple Mac mini computers m General purpose with balanced CPU, memory and storage n Network performance optimzied p General Purpose GPU r Memory-optimized t Burstable instances x Lowest price-per-GB RAM instances z Highest core frequency"},{"location":"aws/services/ec2-pricing/#generational-upgrades","title":"Generational Upgrades","text":"<p>Typically, as cloud infrastructure providers release new generations for families, it's cheaper and more performant to run the later generation instance types. Upgrading instances from one generation to another can be a major area of cost savings. Generation upgrades usually result in between 5% and 10% cost savings per generation and vary per family. </p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Aug 14, 2021</p>"},{"location":"aws/services/ecr-pricing/","title":"ECR Pricing | Cloud Cost Handbook","text":"<p>Amazon ECR Pricing Page</p>"},{"location":"aws/services/ecr-pricing/#summary","title":"Summary","text":"<p>Amazon Elastic Container Registry (ECR) is a fully managed container registry that allows you to store container images. You can create as many repositories as you'd like that are free. As you push container images to your repository, you're charged for the storage of these images, which can accrue over time. Additionally, ECR charges different data transfer rates for private vs public repositories. </p>"},{"location":"aws/services/ecr-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Container Image Storage ECR charges a rate per month for the amount of storage per GB you store for container images. Data Transfer ECR charges different rates for data transfer from public and private repositories."},{"location":"aws/services/ecr-pricing/#storage-costs-per-ecr-repository","title":"Storage Costs per ECR Repository","text":"<p>Determining the cost per Container Repository can be a lot of effort, especially if you have a large quantity of images. To calculate the storage cost per container repository:</p> <ul> <li>List all of your container images.</li> <li>Collect the image digest from each.</li> <li>Determine just the unique digests of the layers in your container repository.</li> <li>Get the size of each unique digest.</li> </ul> <p>If you prefer not to do this manually yourself, Vantage will compute the size and corresponding cost of all repositories automatically when you connect an AWS account.</p>"},{"location":"aws/services/ecr-pricing/#lifecycle-policies","title":"Lifecycle Policies","text":"<p>ECR stores every container image you push to a registry by default. Over time, the storage of all of these images can add up. Amazon offers a primitive called a lifecycle Policy that allows you to set conditions for having Amazon clean up images on your behalf. There are two types of lifecycle policies:</p> Lifecycle Policy Description <code>imageCountMoreThan</code> ECR allows you to define a certain number of images to retain and anything over that count will be cleaned up. For example, if you set a lifecycle Policy with a <code>imageCountMoreThan</code> value of 10, your most recent 10 images will always be kept. <code>sinceImagePushed</code> ECR allows you to set lifecycle policies with a value of <code>sinceImagePushed</code>, which has a value of a certain number of days. So, for example, if you have a lifecycle Policy applied with a <code>sinceImagePushed</code> value of seven, ECR will delete images when they are older than seven days. <p>Note</p> <p>When you apply a lifecycle Policy, it is evaluated immediately. So, if you have 500 images in a repository and impose a lifecycle policy of 10, as soon as that policy is applied ECR will delete the 490 oldest images. </p>"},{"location":"aws/services/ecr-pricing/#example-imagecountmorethan-lifecycle-policy","title":"Example <code>imageCountMoreThan</code> Lifecycle Policy","text":"<p>Here's an example of how to impose a lifecycle Policy via the AWS CLI using the value of <code>imageCountMoreThan</code>: </p> <pre><code>aws ecr put-lifecycle-policy \\\n    --repository-name \"vantage/mcyolo\" \\\n    --lifecycle-policy-text \"file://policy.json\"\n</code></pre> <p>Where the content of the file for policy.json is the following:</p> <pre><code>{\n  \"rules\": [\n     {\n       \"rulePriority\": 1,\n       \"description\": \"Expire images over a count of 10\",\n       \"selection\": {\n         \"tagStatus\": \"untagged\",\n         \"countType\": \"imageCountMoreThan\",\n         \"countNumber\": 10\n       },\n       \"action\": {\n         \"type\": \"expire\"\n       }\n     }\n  ]\n}\n</code></pre>"},{"location":"aws/services/ecr-pricing/#example-sinceimagepushed-lifecycle-policy","title":"Example <code>sinceImagePushed</code> Lifecycle Policy","text":"<p>Here's an example of how to impose a Lifecycle Policy via the AWS CLI using the value of sinceImagePushed: </p> <pre><code>aws ecr put-lifecycle-policy \\\n    --repository-name \"vantage/mcyolo\" \\\n    --lifecycle-policy-text \"file://policy.json\"\n</code></pre> <p>Where the content of the file for policy.json is the following:</p> <pre><code>{\n  \"rules\": [\n     {\n       \"rulePriority\": 1,\n       \"description\": \"Expire images older than 14 days\",\n       \"selection\": {\n         \"tagStatus\": \"untagged\",\n         \"countType\": \"sinceImagePushed\",\n         \"countUnit\": \"days\",\n         \"countNumber\": 14\n       },\n       \"action\": {\n         \"type\": \"expire\"\n       }\n     }\n  ]\n}\n</code></pre> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 11, 2021</p>"},{"location":"aws/services/ecs-and-fargate-pricing/","title":"ECS & Fargate Pricing | Cloud Cost Handbook","text":"<p>ECS Pricing Page Fargate Pricing Page</p>"},{"location":"aws/services/ecs-and-fargate-pricing/#summary","title":"Summary","text":"<p>Amazon Elastic Container Service (ECS) allows you to run docker containers through a primitive named a task. Tasks ultimately run on EC2 instances which are either managed by you (ECS on EC2) or fully managed by AWS (Fargate).</p> <p>There is no additional charge to you when using ECS on self-managed EC2 as you're just paying for EC2 instances that you create and manage. Fargate charges you for the vCPU and Memory for an ECS task or EKS Pod and you pay a premium for managing the underlying EC2 instances. </p>"},{"location":"aws/services/ecs-and-fargate-pricing/#fargate-pricing-dimensions","title":"Fargate Pricing Dimensions","text":"Dimension Description vCPU Hours When configuring a Fargate task or EKS Pod you assign a certain amount of vCPU and are charged a corresponding per hour vCPU rate. GB Memory Hours When configuring a Fargate task or EKS Pod you assign a certain amount of GB of memory and are charged a corresponding per hour GB of memory rate."},{"location":"aws/services/ecs-and-fargate-pricing/#fargate-spot","title":"Fargate Spot","text":"<p>Fargate has the ability to run in a Spot capacity which is conceptually the same premise as EC2 Spot, allowing you to run tasks at up to a 70% discount off the Fargate on-demand price. </p> <p>When the capacity for Fargate Spot is available, you will be able to launch tasks based on your specified request. When AWS needs the capacity back, tasks running on Fargate Spot will be interrupted with two minutes of notification. If the capacity for Fargate Spot stops being available, Fargate will scale down tasks running on Fargate Spot while maintaining any regular tasks you are running.</p>"},{"location":"aws/services/ecs-and-fargate-pricing/#fargate-vs-self-managed-ec2-on-ecs-or-eks","title":"Fargate vs Self-Managed EC2 on ECS or EKS","text":"<p>Fargate charges a significant premium for managing the underlying nodes. Additionally, Fargate has varying degrees of vCPU performance that differ depending on the task. As a result, Fargate can have pitfalls relative to self-managed ECS or EKS on EC2 beyond just the additional costs. </p> <p>For a more in-depth article for seeing how Fargate is priced relative to self-managed EC2, please read the following blog post: AWS Fargate Pricing Explained.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Aug 8, 2021</p>"},{"location":"aws/services/efs-pricing/","title":"EFS Pricing | Cloud Cost Handbook","text":"<p>Amazon EFS Pricing Page</p>"},{"location":"aws/services/efs-pricing/#summary","title":"Summary","text":"<p>Amazon Elastic File System (EFS) is a scalable elastic file storage system, where workloads are scaled up and down automatically as files are added and removed. Some use cases include containerized and serverless applications, big data analytics, development and testing, database backups, and machine learning training.</p>"},{"location":"aws/services/efs-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Storage Classes See the Storage Classes section for more information. File Systems There are two file systems to choose from\u2014EFS Regional File System (Multi-AZ) and EFS One Zone. With the Regional File System option, files are stored across at least three Availability Zones (AZ). For files where availability and durability are less important, One Zone stores files in just one AZ within an AWS region, at a much lower storage price. Throughput Modes There are two throughput modes\u2014Elastic Throughput mode and Provisioned Throughput mode. Elastic Throughput mode is recommended for unpredictable peak throughput needs or spiky throughput usage. Use Provisioned Throughput for high peak throughput capacity. Elastic Throughput mode charges for reads and writes per GB transferred whereas Provisioned Throughput charges are based on MB/s. Also, Infrequent Access storage is more expensive using Provisioned Throughput. Storage Charges for storage vary depending on your region, as wall as your choice of storage class, file system, and throughput mode. Data Transfer With Elastic Throughput mode, you are charged for reads and writes per GB transferred. You are also charged for tiering between storage classes. Charges for writes are more expensive in the cost-optimized storage classes."},{"location":"aws/services/efs-pricing/#storage-classes","title":"Storage Classes","text":"<p>Amazon EFS offers three storage classes, each with different pricing rates and functionality. Availability and cost are the tradeoffs, the more available the data is, the higher the storage costs are. Each EFS storage class is described below:</p> Storage Class Description EFS Standard Standard is the high-speed, low-latency option for regularly accessed or modified data workloads. EFS Infrequent Access Providing the same features, durability, throughput, and IOPS scalability as Standard, the Infrequent Access class is ideal for workloads where the \u201csub-millisecond latencies\u201d of Standard are not needed. Use this class for data that is accessed a few times a quarter. EFS Archive Just like Infrequent Access, Archive provides the same features, durability, throughput, and IOPS scalability as Standard. Archive is recommended for workloads where data is accessed a few times a year. <p>Make use of EFS Lifecycle Management to set lifecycle policies to transition files into lower-cost storage classes after periods of no use.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Feb 5, 2024</p>"},{"location":"aws/services/elasticache-pricing/","title":"ElastiCache Pricing | Cloud Cost Handbook","text":"<p>Amazon ElastiCache Pricing Page</p>"},{"location":"aws/services/elasticache-pricing/#summary","title":"Summary","text":"<p>Amazon ElastiCache allows you to set up, run, and scale popular open-source compatible in-memory data stores, like Redis or Memcached. ElastiCache ultimately runs atop EC2 instances with pre-configured software, is prefixed with <code>cache</code>, and is referred to as Nodes.</p>"},{"location":"aws/services/elasticache-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Node Usage You are charged for the amount of hours your ElastiCache nodes run."},{"location":"aws/services/elasticache-pricing/#reserved-instances","title":"Reserved Instances","text":"<p>ElastiCache Nodes do have Reserved Instances that can give you significant savings. Reserved Instances are covered as a general concept found here. </p> <p>Typically, as ElastiCache Nodes remain on for longer durations and aren't members of Auto Scaling groups, they are good candidates for cost savings via Reserved Instances. </p>"},{"location":"aws/services/elasticache-pricing/#savings-plans","title":"Savings Plans","text":"<p>ElastiCache Nodes are not covered under AWS Savings Plans.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 11, 2021</p>"},{"location":"aws/services/elasticsearch-pricing/","title":"Elasticsearch Service Pricing | Cloud Cost Handbook","text":"<p>Amazon Elasticsearch Service Pricing Page</p>"},{"location":"aws/services/elasticsearch-pricing/#summary","title":"Summary","text":"<p>Amazon Elasticsearch Service is a fully managed service that runs Elasticsearch which is used primarily for querying JSON-based search and analytics data. Amazon Elasticsearch Service is billed per instance for the amount of EBS storage attached to the instance and the type of instance that is used to run the service.</p>"},{"location":"aws/services/elasticsearch-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Instance Type Usage Elasticsearch instance types are billed at an hourly rate and charged that hourly rate on a per second basis for your usage. Attached Storage Elasticsearch allows you to attach storage to the instances either as General Purpose Storage or Provisioned IOPS storage. Behind the scenes these are just managed EBS Volumes that Elasticsearch orchestrates on your behalf. However, on your monthly AWS bill these charges will show up under the Elasticsearch service and not under the EBS service. There are also options for high performance local SSD disks for storage optimized instances."},{"location":"aws/services/elasticsearch-pricing/#storage-optimized-instances","title":"Storage Optimized Instances","text":"<p>If better storage performance, above EBS, is needed you can select Storage Optimized instances which include local NVMe SSD disks.</p>"},{"location":"aws/services/elasticsearch-pricing/#reserved-instances","title":"Reserved Instances","text":"<p>As Elasticsearch instances are not covered by AWS Savings Plans, you must rely on procuring Reserved Instances specifically for Elasticsearch. Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 19, 2021</p>"},{"location":"aws/services/elb-pricing/","title":"Elastic Load Balancer Pricing | Cloud Cost Handbook","text":"<p>Amazon ELB Pricing Page</p>"},{"location":"aws/services/elb-pricing/#summary","title":"Summary","text":"<p>Amazon Elastic Load Balancer (ELB) is a service that distributes traffic from a single endpoint (public or private) to one or many private resources. Most commonly, an Elastic Load Balancer will be exposed to the public internet and will distribute the incoming traffic to several app servers (usually running on EC2 or ECS). Elastic Load Balancers can also be used to distribute private traffic from one service to another. There are different options for the type of ELB and they are priced differently and come with different feature sets.</p>"},{"location":"aws/services/elb-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Load Balancer Hours Every type of load balancer has a standard per hour rate and is always billed for a full hour. Load Balancer Data Processed Each type of load balancer has a formula for how the data processed by the load balancer is turned into an additional hourly charged."},{"location":"aws/services/elb-pricing/#application-load-balancer","title":"Application Load Balancer","text":"<p>Application Load Balancers (ALB) are useful for distributing layer 7 (HTTP, HTTPS, gRPC) traffic to application servers or other backends. ALBs have a standard hourly rate per region and a formula for calculating LCU-hours. The dimensions for calculating LCU are:</p> Dimension Description New Connections A single LCU is 25 new connections per second. Active Connections A single LCU is 3,000 active connections per minute. Processed Bytes A single LCU is 1GB per hour for EC2 instances, containers, and IP addresses as targets and 0.4GB per hour for Lambda functions as targets. Rule Evaluations A single LCU is 1,000 rule evaluations per second. <p>Whichever of these dimensions produces the highest LCU for an hour is what is used to create the charge for LCU-hour.</p>"},{"location":"aws/services/elb-pricing/#network-load-balancer","title":"Network Load Balancer","text":"<p>Network Load Balancers (NLB) are used for forwarding layer 4 traffic (TCP, UDP, TLS) to any other resource with an IP address. NLBs have a standard hourly rate per region and a formula for calculating NLCU-hours depending on the type of network traffic. The dimensions for calculating NCLU are:</p> Dimension TCP UDP TLS New Connection or Flow 800 400 50 Active Connection or Flow 100,000 50,000 3,000 Processed Bytes 1GB 1GB 1GB"},{"location":"aws/services/elb-pricing/#gateway-load-balancer","title":"Gateway Load Balancer","text":"<p>Gateway Load Balancers are used to proxy traffic through third-party virtual appliances, which support GENEVE. GLBs have a standard hourly rate per region and a formula for calculating GLCU-hours. The dimensions for calculating GLCU are:</p> Dimension Description New Connections A single LCU is 600 new connections per second. Active Connections A single LCU is 60,000 active connections per minute. Processed Bytes A single LCU is 1GB per hour for EC2 instances, containers, and IP addresses as targets and 0.4GB per hour for Lambda functions as targets."},{"location":"aws/services/elb-pricing/#classic-load-balancer","title":"Classic Load Balancer","text":"<p>Classic load balancers are the original type of load balancer which has since been superseded by ALB and NLB. CLBs support both layer 7 and layer 4 traffic. CLBs have a standard hourly rate per region and a standard per GB rate per region for traffic processed. </p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 30, 2021</p>"},{"location":"aws/services/emr-pricing/","title":"EMR Pricing | Cloud Cost Handbook","text":"<p>Amazon EMR Pricing Page</p>"},{"location":"aws/services/emr-pricing/#summary","title":"Summary","text":"<p>Amazon Elastic Map Reduce (EMR) is software infrastructure for running map reduce and other big data workloads. It supports open-source frameworks like Apache Spark, projects like Hadoop, and SQL tools like Presto.</p> <p>EMR runs on top of EC2 or EKS instances and also has a serverless option. EMR is available for a wide variety of instances which allows for tight optimization of workloads, for example choosing a compute-optimized vs a memory-optimized instance for Spark vs Hive.</p> <p>To see which EC2 instances are available for EMR, you can add the <code>On EMR</code> and <code>EMR Cost</code> columns on ec2instances.info.</p>"},{"location":"aws/services/emr-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"<p>EMR is billed differently based on the underlying compute service.</p> Dimension Description Running on EC2 EMR is billed as an additional cost per hour for the instance. For example, a m6g.16xlarge has an EMR cost of ~$0.60 per hour. Running on EKS Running on EKS involves 2 dimensions: vCPUs and GiB of memory, with a minimum charge of one minute. Serverless Serverless has Compute, Memory, and Storage dimensions."},{"location":"aws/services/emr-pricing/#emr-optimization","title":"EMR Optimization","text":"<p>Every EMR instance above can also be run as a Spot Instance, which is likely to be appropriate for fault-tolerant workloads on EMR. As of 2023, it is also possible to use Spot Fleets with the price-capacity-optimized allocation strategy for running EMR workloads. Lastly, data transfer charges are likely accumulating from the movement of your big data through the EMR system. You can dramatically reduce these charges, or even eliminate them, by connecting to EMR using interface VPC endpoints.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Aug 22, 2023</p>"},{"location":"aws/services/lambda-pricing/","title":"Lambda Pricing | Cloud Cost Handbook","text":"<p>Lambda Pricing Page</p>"},{"location":"aws/services/lambda-pricing/#summary","title":"Summary","text":"<p>AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. You are charged based on the number of requests for your functions and the duration (the time it takes for your code to execute).</p>"},{"location":"aws/services/lambda-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Number of requests You are charged $0.20 per 1M requests to your Lambda functions. Duration of request The price for duration depends on the amount of memory you allocate to your function. You can allocate any amount of memory to your function between 128MB and 10,240MB, in 1MB increments."},{"location":"aws/services/lambda-pricing/#lambda-savings-plans","title":"Lambda Savings Plans","text":"<p>There is one additional option for saving on Lambda that is agnostic to whether you're running on x86 or ARM: Savings Plans.</p> <p>Savings Plans are a more flexible way to get discounts on AWS, and they work across EC1, Lambda, Fargate, and SageMaker. No action is required if you already own a Savings Plan. The discount is applied automatically unless you have maxed out that Savings Plan on another service. Savings Planner can help you decide how much of a Savings Plan to buy so you are not over-committed.</p> <p>If you have existing saving plans in use on Lambda and are looking to make the switch to Graviton1, you should also ensure what impact that will have on your coverage as the discount rates for x86 vs Graviton2 are different.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Sep 26, 2022</p>"},{"location":"aws/services/macie-pricing/","title":"Macie Pricing | Cloud Cost Handbook","text":"<p>Amazon Macie Pricing Page</p>"},{"location":"aws/services/macie-pricing/#summary","title":"Summary","text":"<p>Amazon Macie is a security service for Amazon S3 that uses machine learning to automatically discover, classify, and protect sensitive data in Amazon S3.</p> <p>More specifically, it scans for sensitive data such as personally identifiable information (PII), financial information, and intellectual property.</p> <p>Pricing is based on the number of buckets monitored, the amount of S3 data scanned, and the number of objects discovered automatically when automated data discovery is enabled.</p>"},{"location":"aws/services/macie-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Number of S3 Buckets Monitored or Scanned Amazon Macie charges a fixed monthly cost per bucket that is monitored or scanned S3 Data Scanned Amazon Macie has a tiered per GB charge based on the amount of S3 data scanned for sensitive data. Pricing varies by region. Automated Data Discovery Amazon Macie charges a monthly cost per 100k objects discovered, in addition to scanning, charges when automated data discovery is enabled. Pricing varies by region."},{"location":"aws/services/macie-pricing/#macie-considerations","title":"Macie Considerations","text":"<p>To minimize costs, consider excluding buckets that are known to not contain sensitive data that would be discovered by Amazon Macie (eg. VPC flow logs, CloudTrail logs and other auto-generated system logs).</p> <p>For large buckets, consider running Macie on a subset of the data using random sampling over a percentage of the objects in a bucket. This is done by specifying the sampling depth when creating a scan.</p>"},{"location":"aws/services/macie-pricing/#organization-accounts","title":"Organization Accounts","text":"<p>When using AWS Organizations there is typically a centralised security or management account which is delegated as the \"Administrator account\" to manage the scans and view the results of Amazon Macie. The Amazon Macie charges will still be associated with the individual organization account the S3 buckets belong to, and not the centralised account being used for the analysis.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Mar 27, 2025</p>"},{"location":"aws/services/rds-pricing/","title":"RDS Pricing | Cloud Cost Handbook","text":"<p>Amazon RDS Pricing Page</p>"},{"location":"aws/services/rds-pricing/#summary","title":"Summary","text":"<p>Amazon Relational Database Service (RDS) provides you with the ability to create databases running certain software such as MySQL, Postgres, SQL Server, and more. RDS instances ultimately are preconfigured EC2 instances running certain managed database software. As a result, you'll see similarities between instance types for RDS and EC2 where RDS instances are prefixed with <code>db</code>.</p>"},{"location":"aws/services/rds-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Instance Type Usage RDS instance types are billed at an hourly rate and charged that hourly rate on a per second basis for your usage. Database Software As RDS allows you to run different types of database software, there are varying costs depending on which database software you choose to use. For example, you can run Oracle and MySQL databases on the same RDS instance types, but they have different pricing, as Oracle licensing contributes a higher cost than MySQL. Availability Zones (AZ) RDS allows you to run RDS instances in either Single-AZ or Multi-AZ deployments. Multi-AZ deployments are more highly available but carry a larger cost. Attached Storage RDS allows you to attach storage to RDS instances either as General Purpose storage or Provisioned IOPS storage. Behind the scenes these are just managed EBS Volumes that RDS orchestrates on your behalf. However, on your monthly AWS bill these charges will show up under the RDS service and not under the EBS service. Backup Storage You have the ability to turn on backups for your RDS instances and are charged an accompanying storage rate for backups."},{"location":"aws/services/rds-pricing/#reserved-instances","title":"Reserved Instances","text":"<p>As RDS instances are not covered by AWS Savings Plans, you must rely on procuring Reserved Instances specifically for RDS. Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information.</p>"},{"location":"aws/services/rds-pricing/#single-az-vs-multi-az","title":"Single-AZ vs Multi-AZ","text":"<p>With RDS, you can choose between a single-AZ or multi-AZ. The benefit of being multi-AZ is that you're provided with enhanced availability and durability for your database as Amazon provisions and maintains a standby in a different availability zone for automatic failover in the event of a scheduled or unplanned outage. </p> <p>From a cost consideration perspective, multi-AZ rates are double what single-AZ rates are for the added durability that you're provided. </p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Aug 10, 2021</p>"},{"location":"aws/services/redshift-pricing/","title":"Redshift Pricing | Cloud Cost Handbook","text":"<p>Amazon Redshift Pricing Page</p>"},{"location":"aws/services/redshift-pricing/#summary","title":"Summary","text":"<p>Redshift is a cloud data warehouse that enables organizations to analyze large volumes of data using SQL queries. The data can be structured and semi-structured across data warehouses, operational databases, and data lakes. With Redshift, you can share and query live data across organizations, accounts, and regions.</p>"},{"location":"aws/services/redshift-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Node Type You are billed at an hourly rate based on your selected node type and node quantity for the duration your cluster is active. The recommended node types for Redshift are RA3 and DC2. Choose based on data size to ensure the best price and performance. If your data is under 1TB uncompressed, it is recommended to use DC2 Node. If your data is currently over 1TB uncompressed or will exceed 1TB in the future, it is recommended to use RA3. Paid Features Additional features can accrue additional costs. Data Transfer Data transfers between Redshift and S3 within the same AWS Region for tasks like backup, restore, load, and unload operations are free of charge. However, any other data transfers into and out of Redshift incur standard AWS data transfer rates. Backup Storage Redshift charges for manual snapshots taken using the console, API, or CLI. This includes manual snapshots taken for RA3 clusters. Storing backups beyond the allocated storage capacity on DC and DS clusters results in additional charges based on the standard S3 storage rates. Should you retain recovery points beyond the initial free 24-hour period, they will lead to additional charges as part of RMS."},{"location":"aws/services/redshift-pricing/#paid-features","title":"Paid Features","text":""},{"location":"aws/services/redshift-pricing/#redshift-serverless","title":"Redshift Serverless","text":"<p>With Redshift Serverless, you can run analytics and scale without setting up and managing warehouse infrastructure. It is ideal for difficult-to-predict compute needs, immediately needed ad-hoc analytics, and test and development environments. </p> <p>You only pay for the capacity used and capacity is automatically scaled up and down depending on need, as well as shutting off during inactivity. Data warehouse capacity is measured in Redshift Processing Units (RPUs). You are billed in RPU hours on a per second basis. Since Redshift Serverless automatically provisions the appropriate resources, you do not need to choose a node type. The features concurrency scaling and Redshift Spectrum are included in the cost.</p>"},{"location":"aws/services/redshift-pricing/#redshift-spectrum","title":"Redshift Spectrum","text":"<p>This feature enables you to execute SQL queries on data stored in S3. The billing is based on the volume of data scanned by Redshift Spectrum, which will be rounded up to the nearest megabyte, with a minimum fee of 10MB per query.</p>"},{"location":"aws/services/redshift-pricing/#redshift-managed-storage","title":"Redshift Managed Storage","text":"<p>Redshift Managed Storage is a feature that allows you to store and manage data within your cluster. It is exclusively available for RA3 node types. Billing is a fixed GB-month rate regardless of data size. Usage of managed storage is computed on an hourly basis, taking into account the total amount of data stored.</p>"},{"location":"aws/services/redshift-pricing/#concurrency-scaling","title":"Concurrency Scaling","text":"<p>Concurrency Scaling is a feature designed to support large numbers of concurrent users and queries. You are charged only for the time queries are actively running. Each cluster receives up to one hour of free Concurrency Scaling credits daily. Any usage that exceeds the free credits is subject to charges on a per second on-demand rate.  </p>"},{"location":"aws/services/redshift-pricing/#redshift-ml","title":"Redshift ML","text":"<p>This functionality enables you to create, train, and deploy machine learning (ML) models. The CREATE MODEL request may accrue additional costs on S3. Charges are typically minimal and should be less than $1 a month. </p>"},{"location":"aws/services/redshift-pricing/#reserved-instances","title":"Reserved Instances","text":"<p>Reserved Instances are covered in depth under General Concepts and we encourage you to read up more on them there for the most up-to-date information.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Sep 19, 2023</p>"},{"location":"aws/services/route-53-pricing/","title":"Route53 Pricing | Cloud Cost Handbook","text":"<p>Route53 Pricing Page</p>"},{"location":"aws/services/route-53-pricing/#summary","title":"Summary","text":"<p>Amazon Route 53 is a Domain Name System (DNS) web service. Typically, Route 53 doesn't tend to be a large cost center for the vast majority of companies. </p>"},{"location":"aws/services/route-53-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Hosted Zones You are charged either $0.50 or $0.10 per hosted zone per month depending on the number of hosted zones you have. You are charged $0.50 per hosted zone per month for the first 25 hosted zones and $0.10 per hosted zone per month for additional hosted zones. DNS Queries You incur charges for every DNS query answered by the Amazon Route 53 service, except for queries to Alias A records that are mapped to Elastic Load Balancing instances, CloudFront distributions, AWS Elastic Beanstalk environments, API Gateways, VPC endpoints, or Amazon S3 website buckets, which are provided at no additional charge. Registered Domain Names You pay an annual charge for each domain name registered via or transferred into Route 53. <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 11, 2021</p>"},{"location":"aws/services/s3-pricing/","title":"S3 Pricing | Cloud Cost Handbook","text":"<p>Amazon S3 Pricing Page</p>"},{"location":"aws/services/s3-pricing/#summary","title":"Summary","text":"<p>Amazon Simple Storage Service (S3) is an object storage service that allows customers to store files called objects. Objects are organized into namespaces called buckets at no additional cost. Ultimately, you are charged on the dimensions below, which are a mix of how much you store with specific storage types, the bandwidth for accessing those files, and the requests you make to the S3 service. </p>"},{"location":"aws/services/s3-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Object Storage Amount AWS charges you for how much you store across all objects and across all buckets. Each region has a different pricing rate on a per GB basis, and as you store more data on S3, you may get discounts on a per GB basis. Object Storage Class Amazon S3 has many different storage classes, further discussed below. Standard is the default storage class, but you can get lower rates for some other tiers. Bandwidth AWS charges you for the amount of egress you consume for accessing S3 objects. You should keep an eye on how much bandwidth is being consumed\u2014that's where you can potentially have runaway costs with significant use. Request Metrics AWS charges you for <code>GET</code>, <code>SELECT</code>, <code>PUT</code>, <code>COPY</code>, <code>POST</code>, and <code>LIST</code> requests. S3 also charges you different rates depending on which of these request types you're using. This is oftentimes an unknown cost that occurs and that you should keep an eye on."},{"location":"aws/services/s3-pricing/#intelligent-tiering","title":"Intelligent-Tiering","text":"<p>S3 Intelligent-Tiering is an Amazon S3 storage class that will automatically optimize storage costs on your behalf. S3 Intelligent-Tiering will monitor access patterns of S3 objects and shift them between five different storage classes to deliver you automatic savings. </p> <p>Typically, customers have files stored in S3 Standard storage, but they may not think to ever optimize these costs and overpay for the number of files they're storing in S3. By using Intelligent-Tiering, you can focus on your application development and allow S3 Intelligent-Tiering to manage shifting their objects' storage classes on your behalf. </p>"},{"location":"aws/services/s3-pricing/#understanding-storage-classes","title":"Understanding Storage Classes","text":"<p>S3 currently supports 28 different object storage types within an S3 bucket. Each bucket is capable of holding objects from a single class or multiple classes. A light overview of these storage types is included below:</p> Storage Type Description Standard Storage (StandardStorage) Standard Storage is for general purpose storage of any type of data, typically used for frequently accessed data. Standard Storage is priced on a tiered basis where it gets incrementally cheaper to store data as you store more. Intelligent-Tiering - Frequent Access (IntelligentTieringFAStorage) Objects uploaded to Intelligent-Tiering are automatically stored in the Frequent Access tier which has the same rates as Standard Storage. Intelligent-Tiering - Infrequent Access (IntelligentTieringIAStorage) Objects in Frequent Access that haven't been accessed in 30 consecutive days are moved to this tier in which prices drop significantly. Intelligent-Tiering - Archive Instant Access (IntelligentTieringAIAStorage) Objects that haven\u2019t been accessed in 90 consecutive days are moved to this tier in which prices drop even more. Intelligent-Tiering - Archive Access (IntelligentTieringAAStorage) Upon activating the Archive Access tier for Intelligent-Tiering, S3 will automatically move objects that haven\u2019t been accessed for 90 days (or more depending on your configuration) to Archive Access where the pricing is the same as Glacier. Intelligent-Tiering - Deep Archive Access (IntelligentTieringDAAStorage) Upon activating the Deep Archive Access tier for Intelligent-Tiering, S3 will automatically move objects that haven\u2019t been accessed for 180 days (or more depending on your configuration) to Deep Archive Access. Intelligent-Tiering - Archive Access Object Overhead (IntAAObjectOverhead) For each object that is stored in the Intelligent-Tiering - Archive Access tier, 32KB of chargeable overhead is added for index and related metadata, charged at Glacier Flexible Retrieval rates. Intelligent-Tiering - Archive Access S3 Object Overhead (IntAAS3ObjectOverhead) Intelligent-Tiering - Archive Access also requires an additional 8KB of data per object for the name of the object and other metadata, charged at Standard Storage rates Intelligent-Tiering - Deep Archive Access Object Overhead (IntDAAObjectOverhead) For each object that is stored in the Intelligent-Tiering - Deep Archive Access tier, 32KB of chargeable overhead is added for index and related metadata, charged at Glacier Flexible Retrieval rates. Intelligent-Tiering - Deep Archive Access S3 Object Overhead (IntDAAS3ObjectOverhead) Intelligent-Tiering - Deep Archive Access also requires an additional 8KB of data per object for the name of the object and other metadata, charged at Standard Storage rates. Standard - Infrequent Access (StandardIAStorage) Standard - Infrequent Access is for data that is accessed less frequently but requires rapid access when needed. It offers the high durability, high throughput, and low latency of Standard Storage, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes Standard - Infrequent Access ideal for long-term storage, backups, and as a data store for disaster recovery files. Standard - Infrequent Access Size Overhead (StandardIASizeOverhead) There is a minimum billable object size of 128KB. For example, if you stored an object at 28KB, the rate would increase by 100KB, (128KB - 28KB) and is represented by this metric. Standard - Infrequent Access Object Overhead (StandardIAObjectOverhead) For each object stored in Standard - Infrequent Access 32KB of chargeable overhead is added for metadata. Standard - Infrequent Access (One Zone) (OneZoneIAStorage) Standard - Infrequent Access (One Zone) is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 storage classes which store data in a minimum of three Availability Zones (AZ), Standard - Infrequent Access (One Zone) stores data in a single AZ and costs much less than Standard - Infrequent Access. One Zone Size Overhead (OneZoneIASizeOverhead) There is a minimum billable object size of 128KB. For example, if you stored an object at 28KB, the rate would increase by 100KB, (128KB - 28KB) and is represented by this metric. Glacier Instant Retrieval (GlacierInstantRetrievalStorage) Glacier Instant Retrieval (GlacierInstantRetrievalStorage) is a high-latency, low-cost, durable archive storage class. The use case is ideal for data that requires long-term storage and is only accessed once per quarter. Glacier Instant Retrieval Size Overhead (GlacierInstantRetrievalSizeOverhead) There is a minimum billable object size of 128KB. For example, if you stored an object at 28KB, the rate would increase by 100KB, (128KB - 28KB) and is represented by this metric. Glacier Flexible Retrieval (GlacierStorage) Glacier Flexible Retrieval (formerly called Glacier) is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, Glacier provides three retrieval options that range from a few minutes to hours. Glacier Overhead (GlacierObjectOverhead) For each object that is stored in Glacier, 32KB of chargeable overhead is added for index and related metadata, charged at Glacier Flexible Retrieval rates. Glacier S3 Object Overhead\u00a0(GlacierS3ObjectOverhead) Glacier also requires an additional 8KB of data per object for the name of the object and other metadata, charged at Standard Storage rates. Glacier Staging Storage\u00a0(GlacierStagingStorage) Staging storage serves as the temporary holding space for the components of a Multipart Upload until the CompleteMultipart request is initiated. These parts are temporarily stored in Standard Storage, and chargers based on Standard Storage pricing. Glacier Deep Archive (DeepArchiveStorage) Glacier Deep Archive (DeepArchiveStorage) is tied with Intelligent-Tiering - Deep Archive Access as Amazon S3\u2019s lowest-cost storage class. It supports long-term retention and digital preservation of data that may be accessed once or twice a year. It is designed for customers\u201a particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors, that retain data sets for 7-10 years or longer to meet regulatory compliance requirements. Glacier Deep Archive can also be used for backup and disaster recovery use cases, and is a cost-effective and easy-to-manage alternative to magnetic tape systems, whether they are on-premises libraries or off-premises services. Deep Archive Object Overhead (DeepArchiveObjectOverhead) For each object that is stored in Glacier Deep Archive, 32KB of chargeable overhead is added for index and related metadata, charged at Glacier Deep Archive rates. Deep Archive S3 Object Overhead (DeepArchiveS3ObjectOverhead) Glacier Deep Archive also requires an additional 8KB of data per object for the name of the object and other metadata, charged at Standard Storage rates. Deep Archive Staging Storage (DeepArchiveStagingStorage) Staging storage is where the parts of Multipart Upload are staged until the CompleteMultipart request is issued. The parts are staged in Standard Storage, and storage is charged at the Standard Storage price. S3 Reduced Redundancy Storage is an Amazon S3 storage option that enables customers to store noncritical, reproducible data at lower levels of redundancy than Standard Storage. It provides a highly available solution for distributing or sharing content that is durably stored elsewhere, or for storing thumbnails, transcoded media, or other processed data that can be easily reproduced. The Reduced Redundancy Storage option stores objects on multiple devices across multiple facilities, providing 400 times the durability of a typical disk drive, but does not replicate objects as many times as Standard Storage. Express One Zone (ExpressOneZone) Express One Zone, like Standard - Infrequent Access (One Zone) is a single AZ storage class. It can provide extremely quick, single-digit millisecond access to your data at a lower price than Standard Storage. Some examples of use cases are Machine Learning and Financial Modeling. Outposts (Outposts) AWS Outposts extend AWS services, tools, etc to your on-premises AWS Outposts environment. Ideal for locally required data, with S3 on Outposts you can reliably store and access data on your Outpost."},{"location":"aws/services/s3-pricing/#s3-bucket-request-metrics","title":"S3 Bucket Request Metrics","text":"<p>S3 does not have ingress, egress, or request metrics turned on by default, leaving many users unsure of what their costs will be until they receive their monthly AWS bill. That being said, it's relatively easy to enable these metrics. </p> <p>Below is an example of how to enable these metrics for a S3 bucket via the AWS CLI. Just be sure to replace <code>YOUR_BUCKET_NAME</code> with your actual bucket name and <code>YOUR_BUCKET_REGION</code> with the appropriate bucket region.</p> <pre><code>aws s3api put-bucket-metrics-configuration \n  --bucket YOUR_BUCKET_NAME\n  --metrics-configuration Id=EntireBucket \n  --id EntireBucket \n  --region YOUR_BUCKET_REGION\n</code></pre> <p>Note</p> <p>It takes roughly 15 minutes for AWS to begin delivering these metrics after being enabled.</p>"},{"location":"aws/services/s3-pricing/#s3-vs-cloudflare-bandwidth-alliance-partner","title":"S3 Vs Cloudflare Bandwidth Alliance Partner","text":"<p>The Cloudflare Bandwidth Alliance is a group of infrastructure providers that have decided to either completely waive or massively discount egress fees for shared customers. This can be a huge source of savings for customers who have an AWS bill where S3 egress costs make up a large portion of the aforementioned bill.</p> <p>By moving S3 content to Cloudflare's content delivery network (CDN) service in tandem with a Bandwidth Alliance provider, you can get no-cost content transit from their Cloudflare origin server to Cloudflare servers distributed around the world. This effectively reproduces the cost benefit that users get for pairing CloudFront with an AWS service, like S3.<sup>1</sup> Utilizing one of Cloudflare's self-serve plans, you can also cap their cost to deliver content via flat-rate pricing. Further details can be found in the CloudFront section of the Cloud Cost Handbook.</p>"},{"location":"aws/services/s3-pricing/#considerations","title":"Considerations","text":"<p>Price is not the only consideration that goes into making a decision about whether to utilize S3 or a competing storage service. Performance, availability, user experience, support, and legal compliance are other factors that will factor into the decision to utilize one service over another.</p>"},{"location":"aws/services/s3-pricing/#complexity","title":"Complexity","text":"<p>AWS has made it exceedingly easy for customers to utilize other AWS services in tandem, but there is a non-trivial cost for an organization to decide to split their infrastructure over multiple service providers. Developers will need to learn and understand both systems and when to choose one design pattern over the other. There will be two sets of documentation that will need to be addressed when designing or troubleshooting systems.</p>"},{"location":"aws/services/s3-pricing/#use-cases","title":"Use Cases","text":"<p>The primary use case in favor of utilizing this cost-efficiency architecture strategy is if a user has a large amount of static content that is stored on S3 and being served to end-users via the internet.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Feb 2, 2024</p> <ol> <li> <p>\"If you are using an AWS origin, effective December 1, 2014, data transferred from origin to edge locations (Amazon CloudFront origin fetches) will be free of charge.\" https://aws.amazon.com/cloudfront/pricing/\u00a0\u21a9</p> </li> </ol>"},{"location":"aws/services/vpc-flow-logs-pricing/","title":"VPC Flow Logs Pricing | Cloud Cost Handbook","text":"<p>Amazon VPC Flow Logs Pricing Page</p>"},{"location":"aws/services/vpc-flow-logs-pricing/#summary","title":"Summary","text":"<p>VPC Flow Logs is an AWS feature that records inbound and outbound IP traffic information from your Virtual Private Cloud (VPC). Flow logs can be used for many uses cases, including:</p> <ul> <li>Monitor network traffic patterns between source and destination.</li> <li>Review traffic flows for compliance requirements and security reviews.</li> <li>Monitor for performance and potential bottlenecks. You can review for bytes and packets transferred, latency, and protocol distribution (e.g., percentage of traffic using TCP, UDP, etc.) and packet loss.</li> </ul> <p>VPC Flow Logs capture metadata about the traffic flow, such as protocol, bytes, action (e.g., accept/reject), type of traffic (e.g., IPv4, IPv6), ports, packets transferred, and more.</p> <p>Logs can be stored in the following locations:</p> <ul> <li>Amazon CloudWatch Logs</li> <li>Amazon S3</li> <li>Amazon Data Firehose</li> </ul>"},{"location":"aws/services/vpc-flow-logs-pricing/#vpc-flow-logs-considerations","title":"VPC Flow Logs Considerations","text":"<p>Consider the following options when you create flow logs:</p> <ul> <li>Data Volume: Once you enable flow logs, the traffic can generate large volumes of data, leading to high costs. When creating flow logs, you can filter on whether you want to log accepted, rejected, or all traffic.</li> <li>Custom or Default Format: The default format includes a set of predefined fields. If you want to define different fields or order of fields, select a custom format for logs.</li> <li>File Format: When you create a flow log, you have the option to specify either text (default) or Parquet file formats. Parquet compressed with Gzip takes up 20% less strorage\u2014and therefore less storage costs\u2014than Gzip text format.</li> <li>Aggregation Interval: When you create a flow log, you can add the aggregation interval, which is the timeframe for which the traffic flow is analyzed and recorded into a flow log. According to AWS, once this data is captured, \u201cThe flow log service typically delivers logs to CloudWatch Logs in about 5 minutes and to Amazon S3 in about 10 minutes.\u201d</li> <li>Storage Location: Consider aggregating all logs and storing them in a single S3 bucket to reduce costs. Consider options for long-term storage, like Glacier storage classes, which can help you save on costs.</li> </ul> <p>After you create flow logs, consider the following points:</p> <ul> <li>Permissions: Ensure that the IAM roles and permissions are correctly set up to allow VPC Flow Logs to write to CloudWatch Logs, S3, or Kinesis. Incorrect permissions might result in log delivery failures.</li> <li>Latency and Delivery Time: When you first create a flow log, it can take some time for data to start being published to the chosen destination. These logs are not sent in real time. There also might be a delay between when traffic occurs and when flow logs are available for analysis.</li> <li>Unlogged Traffic: Not all network traffic is logged. For example, traffic that flows between an endpoint network interface and a Network Load Balancer network interface is not logged. DHCP traffic and mirrored traffic are also not logged.</li> <li>Cost Allocation Tagging: Consider using tags to identify metadata or owners for logs and costs. This can help with determining which teams are responsible for costly logs. See the AWS documentation for details.</li> </ul>"},{"location":"aws/services/vpc-flow-logs-pricing/#vpc-flow-logs-pricing-dimensions","title":"VPC Flow Logs Pricing Dimensions","text":"<p>With regard to pricing, VPC Flow Logs fall into the category of Vended logs. Vended logs are \u201clogs natively published by AWS services on behalf of the customer and available at volume discount pricing.\u201d Pricing is mostly based on the amount of data captured. The full cost is determined by the volume of log data and the destination for storage (i.e., CloudWatch Logs, S3, or Data Firehose).</p> Dimension Description Data Ingestion Pricing varies based on ingestion location\u2014CloudWatch Logs, S3, and Data Firehose. For CloudWatch Logs, there are Standard and Infrequent Access options. Charges are based on the amount of data ingested. Per GB charges decrease as the amount of data ingested increases. Storage Charges The cost of storing the log data. For CloudWatch Logs, you\u2019re charged per GB per month. For S3, charges are per GB per month for data stored, with different rates for Standard and Glacier storage classes. <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jun 14, 2024</p>"},{"location":"aws/services/vpc-pricing/","title":"VPC Pricing | Cloud Cost Handbook","text":"<p>Amazon VPC Pricing Page</p>"},{"location":"aws/services/vpc-pricing/#summary","title":"Summary","text":"<p>Amazon Virtual Private Cloud (VPC) is a service that allows customers to logically isolate their resources into different networks. Unless explicitly configured, every VPC is completely isolated from every other VPC. There is no charge for a VPC in itself, however some optional sub-components of a VPC can incur charges.</p>"},{"location":"aws/services/vpc-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description NAT Gateway Usage NAT Gateways are billed per hour at a standard rate per region. Each partial hour is billed as a full hour. NAT Gateway Transfer NAT Gateways are billed per GB which is processed by the gateway regardless of where the data is being transferred to or from."},{"location":"aws/services/vpc-pricing/#nat-gateway","title":"NAT Gateway","text":"<p>NAT (Network Address Translation) Gateways enable resources running inside of VPCs to connect to services outside of the VPC without needing to expose those resources to the public internet. Besides the standard usage and transfer charges on NAT Gateways, you will also be charged standard bandwidth transfer charges on top of that depending on where the traffic is going.</p>"},{"location":"aws/services/vpc-pricing/#amazon-vpc-endpoints","title":"Amazon VPC Endpoints","text":"<p>VPC Endpoints allow resources to connect to other AWS services outside of a VPC, such as S3, without the need for a NAT Gateway. This is a good way to prevent NAT Gateway usage and transfer charges.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 19, 2021</p>"},{"location":"aws/services/workspaces-pricing/","title":"WorkSpaces Pricing | Cloud Cost Handbook","text":"<p>Amazon WorkSpaces Pricing Page</p>"},{"location":"aws/services/workspaces-pricing/#summary","title":"Summary","text":"<p>Amazon WorkSpaces is a fully managed, persistent desktop virtualization service. You can use Amazon WorkSpaces to provision either Windows or Linux desktops, each with its own set of pricing implications discussed below. WorkSpace pricing can either be done in a monthly or hourly fashion. </p>"},{"location":"aws/services/workspaces-pricing/#pricing-dimensions","title":"Pricing Dimensions","text":"Dimension Description Compute Type WorkSpaces offers seven different types of compute types. They are <code>Value</code>, <code>Standard</code>, <code>Performance</code>, <code>Power</code>, <code>PowerPro</code>, <code>Graphics</code>, and <code>GraphicsPro</code>. Each of these classes has a different set of underlying resources that contribute to costs differently. The order that these classes are listed in are from cheapest to most expensive. Platform Type Linux or Windows. You are charged an additional amount of money for running on Windows vs Linux. You may also bring your own license for Windows WorkSpaces to reduce Windows licensing costs if you have that available. Running Mode <code>AUTO_STOP</code> or <code>ALWAYS_ON</code>. When you choose <code>AUTO_STOP</code> you are choosing to create a WorkSpace that has a pre-determined expiration time in which that WorkSpace will terminate and is billed per hour. When you choose <code>ALWAYS_ON</code> you are charged on a monthly rate basis and the WorkSpace will persist being on until you take action to terminate it. WorkSpace Size Each compute type offers four different configurations with different amounts of vCPU and GB of Memory. <code>Graphic</code> and <code>GraphicsPro</code> only offer one size. Depending on the size you choose, you will pay a more expensive rate."},{"location":"aws/services/workspaces-pricing/#monitoring-unused-workspaces","title":"Monitoring Unused WorkSpaces","text":"<p>WorkSpaces have an attribute named <code>last_known_user_connection_timestamp</code> that maintains a timestamp of the last time a user accessed a specific WorkSpace. You should periodically audit WorkSpaces to ensure that they're being used as otherwise it can be wasteful and a contributor to costs. In the event that this timestamp isn't present, it means that a user has never actually connected to this instance. Additionally, you can look for a certain amount of time that has progressed since a user has accessed it, in the event that a user hasn't accessed a WorkSpace in over a few weeks, it may be a good candidate for a cleanup for cost savings. </p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Jul 28, 2021</p>"},{"location":"datadog/committed-use-discounts/","title":"Datadog Committed Use Discounts | Cloud Costs Handbook","text":"<p>Datadog Volume Discount Reference</p> <p>You may contact your Datadog account manager to realize discounts from on-demand spend. Datadog offers monthly minimum usage commitments for at least the following services:</p> <ul> <li>Infrastructure</li> <li>Log Management</li> <li>APM</li> <li>Database Monitoring</li> <li>Cloud Security Management</li> </ul> <p>20-50% savings from your variable usage plans are possible. Note that variable usage plans are still billed annually but a minimum commitment will result in greater savings. Datadog does not publicly share all of the rates for minimum commitments.</p> <p>One example of container monitoring states:</p> <p>Additional containers will be billed at $0.002 per container per hour. In addition, you can purchase prepaid containers at $1 per container per month.</p> <p>In a month that has 744 hours, the on-demand cost of a container will be $1.488 whereas a committed container would be $1.00 which represents a 32.8% discount. </p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"kubernetes/kubernetes-rightsizing/","title":"Kubernetes Rightsizing | Cloud Cost Handbook","text":"<p>Without proper resource allocation, Kubernetes clusters can become overprovisioned, leading to wasted resources and increased costs. Kubernetes rightsizing is the process of optimizing resource allocation to find a balance between performance, efficiency, and cost-effectiveness. The goal of rightsizing is to match the resources allocated to each workload with the actual resource requirements of the workload, therefore avoiding overprovisioning.</p>"},{"location":"kubernetes/kubernetes-rightsizing/#kubernetes-resource-management","title":"Kubernetes Resource Management","text":"<p>Kubernetes resource management is based on setting resource parameters, like CPU (compute) and memory. Kubernetes orchestrates the allocation of pods onto nodes, based on resource requirements, to ensure efficient utilization of cluster resources. Each container within a Kubernetes pod needs a certain amount of CPU and memory to optimally function. Kubernetes provides mechanisms for specifying resource requests and limits for each container in a pod.</p> <ul> <li>Resource requests: The amount of CPU and memory that a container needs to run. A container running within a pod on a node can use more resources than what is specified in its <code>requests</code>.</li> <li>Resource limits: The maximum amount of CPU and memory that a container can consume. Containers cannot use more resources than what\u2019s specified in its <code>limits</code>.</li> </ul> <p>The <code>requests</code> and <code>limits</code> are set within the resources for each container within a pod as follows.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: demo-pod\nspec:\n  containers:\n    - name: demo-container\n      image: demo-image\n      resources:\n        requests:\n          memory: \"256Mi\"\n          cpu: \"100m\"\n        limits:\n          memory: \"512Mi\"\n          cpu: \"200m\"\n... \n</code></pre>"},{"location":"kubernetes/kubernetes-rightsizing/#the-importance-of-rightsizing","title":"The Importance of Rightsizing","text":"<p>Rightsizing Kubernetes resources is especially important from both a cost and infrastructure perspective.</p> <ul> <li>Cost optimization: Overprovisioned workloads lead to unnecessary expenses since more resources are allocated than what's required.</li> <li>Performance optimization: Applications need to have the necessary computing power and memory to perform efficiently. It\u2019s important to properly allocate resources and avoid issues with your applications\u2019 performance. Containers that try to use more resources than allocated, like memory, could run into errors, like <code>OOMKilled</code>.</li> <li>Scalability: During peak usage periods or times (e.g., increased demand based on seasonality), and when you need resources to scale, resources need to be rightsized to meet changing or increased demands. Understanding these peak usage periods better helps to determine how you should rightsize for the future.</li> </ul>"},{"location":"kubernetes/kubernetes-rightsizing/#kubernetes-rightsizing-process","title":"Kubernetes Rightsizing Process","text":"<p>Kubernetes rightsizing is a continuous process to monitor your infrastructure, collect metrics, adjust resources, and finally, review and reassess. The process repeats itself as you continually monitor changes or spikes in usage.</p> <p></p> <ul> <li>Monitor Infrastructure: You can use Kubernetes monitoring tools, like Prometheus, to gather insights about resource usage patterns. You can also use other tools, like the Vantage Kubernetes agent integration and rightsizing recommendations, to keep an eye on your costs and resource usage over time.</li> <li>Collect Metrics: Review metrics, like average or maximum usage, for each container. Be sure to review current and historical workload data to make decisions on resource allocation. Rightsizing recommendations in Vantage provide a view of your current configuration, average, and maximum usage for CPU and memory. The chart includes a per-day average usage. The table provides a 30-day average and 30-day average max usage. An estimate of potential monthly savings is also provided based on the recommended configuration.</li> <li>Adjust Resources: There are several tools and methods you can use to adjust Kubernetes resources. You can manually adjust resources or consider automated alternatives. Some suggestions are provided below.<ul> <li>Horizontal Pod Autoscaler (HPA): This Kubernetes tool automatically scales workloads based on demands. Horizontal scaling indicates that more pods will be deployed. As each load decreases, the HPA will also scale down the pods within the workload.</li> <li>Vertical Pod Autoscaler (VPA): You can also vertically scale workloads using the VPA. Vertical scaling means you adjust the existing infrastructure to meet demands rather than adding new pods. This project automatically sets requests for containers based on usage.</li> <li>Karpenter: Karpenter is an open-source autoscaler that was designed for AWS but can also be used with other cloud providers. If pods become unschedulable, Karpenter will provision new nodes. It analyzes any pod requirements and provisions nodes to meet those requirements, as well as removes unneeded nodes. Karpenter consistently watches for underutilized nodes to help decrease cluster compute costs.   </li> <li>Quality of Service (QOS): Adjust resource requests and limits to ensure your workloads are assigned the necessary QOS class. This ensures your workloads receive the necessary resources when node resources are exceeded.</li> </ul> </li> <li>Review and Reassess: Implement the above measures and continue to review your infrastructure. Repeat this analysis and respond to changes over time.</li> </ul>"},{"location":"kubernetes/kubernetes-rightsizing/#kubernetes-rightsizing-example","title":"Kubernetes Rightsizing Example","text":"<p>In the following example, a Kubernetes Deployment for a web app consists of a single container running a Node.js server. The initial resource <code>requests</code> and <code>limits</code> for this container are set as follows.</p> <pre><code>...\nspec:\n  containers:\n    - name: nodejs-dev-server\n      image: test-nodejs-image\n      resources:\n        requests:\n          memory: \"1000Mi\"\n          cpu: \"1000m\"\n        limits:\n          memory: \"2000Mi\"\n          cpu: \"2000m\"\n...\n</code></pre> <p>Based on the above configuration:</p> <ul> <li>The container\u2019s <code>requests</code> parameter is set to 1000 MiB of memory and 1000 mCPU to run.</li> <li>The container\u2019s <code>limits</code> parameter is set to 2000 MiB of memory and 2000 mCPU.</li> </ul> <p>Over time, the actual resource utilization is significantly lower than the currently allocated resources. The application, on average, uses about 300 MiB of memory and 200 mCPU under normal operating conditions. In this case, the workload is overprovisioned. The allocated resources exceed the actual requirements of the application, which could lead to higher costs and wasted resources.</p> <p>In the example below, this container is monitored by the Vantage Kubernetes agent and rightsizing recommendations. Average and maximum usage are provided along with the recommended suggestion to rightsize and potential monthly savings.</p> <p></p> <p>To address this overprovisioning, you can rightsize the workload by adjusting the resource <code>requests</code> based on the observed resource utilization. Rightsizing recommendations in Vantage aim for an efficiency target of 80%. This ensures more efficient resource utilization and cost optimization within the Kubernetes cluster and that resources have a reasonable buffer to handle small spikes in usage.</p> <p>You also set <code>limits</code> equal to <code>requests</code>, which corresponds with a QOS class of <code>Guaranteed</code>. Per Kubernetes, the <code>Guaranteed</code> QOS class has the \"strictest resource limits and [is] least likely to face eviction.\" You adjust your resources as follows.</p> <pre><code>...\nspec:\n    containers:\n    - name: nodejs-server\n      image: test-nodejs-image\n      resources:\n        requests:\n          memory: \"375Mi\"\n          cpu: \"250m\"\n        limits:\n          memory: \"375Mi\"\n          cpu: \"250m\"\n...\n</code></pre> <p>With these recommendations in place, you can continue to monitor your usage and adjust accordingly.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p> <p>Last updated Apr 9, 2024</p>"},{"location":"snowflake/adjustments-for-included-cloud-services/","title":"Snowflake Cloud Services and Adjustments | Cloud Costs Handbook","text":"<p>Snowflake Cloud Services Costs Documentation</p> <p>You may have seen an \"Adjustment for Cloud Services\" on a Snowflake bill and wondered why it was negative\u2014or you may be seeing unexpected charges in the \"Cloud Services\" category. Cloud services are a separate pricing dimension for Snowflake that are reported on but not included in your bill, except in certain cases.</p> <p></p>"},{"location":"snowflake/adjustments-for-included-cloud-services/#cloud-services-adjustment-the-10-threshold","title":"Cloud Services Adjustment: The 10% Threshold","text":"<p>Cloud services are services that coordinate various activities across Snowflake. Basically, they're everything that is not involved in running queries or storing data. Most likely, the heaviest cloud service usage will come from:</p> <ul> <li>Metadata management</li> <li>Query parsing and optimization</li> <li>SQL API</li> </ul> <p>Snowflake starts billing for cloud services only after they exceed 10% of your warehouse compute credit cost. Let's say you spend $100 on Snowflake queries and $9 on cloud services. Your total bill will be $100. But if you spent $19 on cloud services, your bill would be $109. This threshold is recalculated every day for the current day's warehouse compute credit usage.</p>"},{"location":"snowflake/adjustments-for-included-cloud-services/#tips-for-reducing-cloud-services-costs","title":"Tips for Reducing Cloud Services Costs","text":"<p>The following common data operations consume cloud services on Snowflake. You can follow recommended patterns to avoid them.</p> <ul> <li>Full clones: Consider selectively cloning your databases for development, ETL, or backup purposes. Cloning consumes only cloud services credits, so if you run a large clone operation on the same day when fewer queries are run, you will pay. Instead, you can clone only the tables you need to stay under the 10% threshold.</li> <li>Fragmented schemas: Snowflake does not recommend using schema design techniques from Hadoop, OLTP, or NoSQL databases where you may have denormalized data spread out across multiple schemas. Instead, use one schema to minimize metadata lookups.</li> <li>Very complex queries: The query optimization software Snowflake runs is broken out into cloud services. So if you write SQL queries that are thousands of lines long, or contain many joins or excessive recursion, you may find yourself with higher cloud services costs.</li> <li>Excessively frequent queries: Lastly, the SQL API handles the ingestion of each SQL query internally. Requesting this API (running queries) tens of thousands of times per day will start to result in charges.</li> </ul> <p>It's possible that these issues may be caused by third-party services running on Snowflake and not your team itself. You can explicitly monitor the queries your company is running by adding query tagging. You can also review additional tips for saving on your Snowflake compute bills.</p> <p></p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"tools/cost-reports/","title":"Cost Reports | Cloud Costs Handbook","text":"<p>Cost Reports are multi-dimensional reports for finding and tracking costs. How to use Cost Reports is covered in the Vantage documentation. This page will focus on a compendium of specific types of reports that are helpful for controlling cloud costs.</p>"},{"location":"tools/cost-reports/#report-examples","title":"Report Examples","text":"<p>These use cases have come up repeatedly in the cloud costs community. Contributors are encouraged to add more as they see fit.</p>"},{"location":"tools/cost-reports/#untagged-resources","title":"Untagged Resources","text":"<p>Many organizations use tags to keep track of all their cloud resources. For practitioners, keeping the percentage of untagged resources low means greater visibility inside cost tools. Tags must be enabled to be used in Cost Reports.</p>"},{"location":"tools/cost-reports/#showback-report","title":"Showback Report","text":"<p>Shared resources like support or a database cluster make divvying up costs among teams difficult. Use the Cost Allocation tool to create a Showback or Chargeback report for transparent reporting.</p>"},{"location":"tools/cost-reports/#compute-costs-without-data","title":"Compute Costs without Data","text":"<p>Very active data teams generate costs as well as insights. For measuring just the cost to deliver software, exclude these costs from a report.</p>"},{"location":"tools/cost-reports/#just-data-egress","title":"Just Data Egress","text":"<p>Data egress is famously the third cost category of AWS that is very significant for teams, alongside compute and storage. Using subcategories, it is possible to see data egress costs per service.</p>"},{"location":"tools/cost-reports/#redshift-instance-costs","title":"Redshift Instance Costs","text":"<p>EC2 instances power multiple managed services inside AWS. To reveal these, save the Report first then click on the drill-down button on any row. In this example, we are inspecting Redshift instance costs.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"},{"location":"tools/instances/","title":"Instances Pricing Documentation | Cloud Cost Handbook","text":"<p>AWS Instance Types Comparison</p> <p>EC2Instances.info shows current pricing for AWS EC2, RDS, and ElastiCache instances. The tool is completely open source and uses the same Amazon APIs available to everyone. Development for Instances is coordinated through the Vantage Slack as well as on GitHub.</p>"},{"location":"tools/instances/#why","title":"Why?","text":"<p>Because it's frustrating to compare instances using Amazon's own instance type, pricing, and other pages.</p>"},{"location":"tools/instances/#columns-and-filters","title":"Columns and Filters","text":"<p>Nearly every service attribute available for a specific instance is available, although most are hidden by default. You can add more attributes, for example, GPUs, by clicking the <code>Columns</code> dropdown. Other dropdowns allow for selecting the <code>Region</code>, changing the per-unit basis of calculation (e.g. vCPUs), and changing the term of the <code>Reserved</code> instance purchase.</p> <p>For each column that is shown, it can be further filtered using simple glob matching, and the entire table can be searched using the top right search box.</p>"},{"location":"tools/instances/#regex-support","title":"Regex Support","text":"<p>Each column and the top right search bar support regex expressions. So you can enter an expression like this: <code>[rt][3456].?.larg</code> and the resulting rows will be a mix of t and r instances as shown above.</p>"},{"location":"tools/instances/#comparing-instances","title":"Comparing Instances","text":"<p>By clicking on an individual row in the table, you can select it to be compared with other rows. You can do this while filtering as well. Click <code>Compare Selected</code> and only the selected rows will be shown. The URL is also changed so this specific comparison can be shared with others.</p>"},{"location":"tools/instances/#detail-pages","title":"Detail Pages","text":"<p>For EC2 and RDS, the <code>API Name</code> column contains clickable links to each instance type. The Detail Page for the instance is essentially a pivot of the main table, with some additional tools to make the information more digestible.</p>"},{"location":"tools/instances/#pricing-widget","title":"Pricing Widget","text":"<p>In the upper left, a pricing widget has selectors for calculating the estimated cost of the instance in different regions, over different amounts of time, or for different software that runs on the instance. When the price is shown as \"N/A\" that indicates that the instance is not available to purchase with the combination of selectors.</p>"},{"location":"tools/instances/#instance-attributes","title":"Instance Attributes","text":"<p>In the middle of each Detail Page are the major categories of attributes and their values. These attributes are all selectable as columns in the main Instances pages. To request more attributes, click <code>Open a ticket</code> in the bottom right.</p>"},{"location":"tools/instances/#saving-and-clearing-filters","title":"Saving and Clearing Filters","text":"<p>Instances automatically saves the filters and selections that are applied to local storage. This means that when you open a new session you will be shown the most recent set of filters and columns. This can be helpful for working on services that mostly use the same types of instances.</p> <p>To reset the table, click <code>Clear Filters</code>.</p>"},{"location":"tools/instances/#export-data","title":"Export Data","text":"<p>The table, with its filters applied, sorted, and with columns shown and hidden, can be exported exactly as a CSV. Data for EC2 is also available for free from the Vantage API.</p>"},{"location":"tools/instances/#azure-support-beta","title":"Azure Support (Beta)","text":"<p>Azure VM types pricing and comparison is also available (Beta). Switch to Azure in the bottom left corner of the page.</p>"},{"location":"tools/instances/#contributors","title":"Contributors","text":"<p>EC2Instances.info was started by @powdahound, contributed to by many, is now managed and maintained by Vantage, and awaits your improvements on GitHub. In the development of Detail Pages, we used components of designs from cloudhw.info with permission from Joshua Powers.</p>"},{"location":"tools/instances/#warning","title":"Warning","text":"<p>EC2Instances.info is not maintained by or affiliated with Amazon. The data shown is not guaranteed to be accurate or current. Please report issues you see.</p> <p>Contribute</p> <p>Contribute to this page on GitHub or join the <code>#cloud-costs-handbook</code> channel in the Vantage Community Slack.</p>"}]}